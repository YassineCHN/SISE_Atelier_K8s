# Kubernetes ‚Äî Fiche 3 : Pratiques avanc√©es

**ConfigMap, Secrets, Stockage persistant, CronJobs et workloads IA**

> üéØ **Fiche facultative**
>
> Cette fiche est destin√©e aux personnes ayant termin√© les fiches 1 et 2 ainsi que les modules MLOps et Data Pipeline. Elle couvre des pratiques avanc√©es couramment utilis√©es en production.

## Objectifs

- Externaliser la configuration avec les **ConfigMaps** et les **Secrets**
- Persister des donn√©es avec les **PersistentVolumeClaims**
- Planifier des t√¢ches batch avec les **CronJobs**
- D√©ployer un stack IA moderne (serveur MCP + agent PydanticAI) sur Kubernetes

> üìù **Pr√©requis**
>
> Fiches 1 et 2 termin√©es. Le projet Iris d√©ploy√© (le backend `mlops-server-svc` doit √™tre actif pour la partie CronJob).

---

## 1. Configuration & Secrets

En production, on ne met jamais de configuration (URL, identifiants, cl√©s API) directement dans le code ou dans les images Docker. Kubernetes propose deux ressources d√©di√©es pour externaliser la configuration :

- **ConfigMap** : stocke des donn√©es de configuration non sensibles (noms, URLs, param√®tres)
- **Secret** : stocke des donn√©es sensibles (mots de passe, tokens, cl√©s API) encod√©es en base64

### a. ConfigMap

#### üìã Exercice ‚Äî Externaliser la configuration du backend Iris

1. Cr√©ez `k8s/config.yaml` :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: iris-config
data:
  API_NAME: "Iris Predictor"
  API_DESCRIPTION: "API de pr√©diction de l'esp√®ce d'iris"
```

Appliquez-le :

```bash
kubectl apply -f k8s/config.yaml
kubectl get configmap iris-config
```

Inspectez son contenu :

```bash
kubectl describe configmap iris-config
```

2. Modifiez `k8s/server.yaml` pour injecter ces valeurs comme variables d'environnement dans le pod :

```yaml
# Dans la section containers du Deployment, ajoutez sous env :
env:
- name: API_URL
  value: "http://mlops-server-svc:8000"
- name: API_NAME
  valueFrom:
    configMapKeyRef:
      name: iris-config          # Nom du ConfigMap
      key: API_NAME              # Cl√© √† injecter
- name: API_DESCRIPTION
  valueFrom:
    configMapKeyRef:
      name: iris-config
      key: API_DESCRIPTION
```

3. Appliquez la mise √† jour :

```bash
kubectl apply -f k8s/server.yaml
```

4. V√©rifiez que les variables sont bien inject√©es en ouvrant un shell dans un pod :

```bash
kubectl exec -it <nom-du-pod> -- /bin/bash
env | grep API
```

Vous devriez voir `API_NAME=Iris Predictor` et `API_DESCRIPTION=API de pr√©diction de l'esp√®ce d'iris`.

Quittez avec `exit`.

> üí° **Pourquoi utiliser un ConfigMap ?**
>
> Si vous avez 10 pods qui utilisent la m√™me configuration, il suffit de modifier le ConfigMap et de red√©ployer ‚Äî pas besoin de rebuilder l'image. C'est le principe de **s√©paration de la configuration et du code**.

---

### b. Secrets

Les Secrets fonctionnent comme les ConfigMaps mais pour les donn√©es sensibles. Kubernetes les stocke encod√©s en base64 et les traite avec plus de pr√©cautions (ils ne s'affichent pas en clair dans les logs).

#### üìã Exercice ‚Äî G√©rer une cl√© API avec un Secret

1. Cr√©ez un Secret depuis la ligne de commande (la m√©thode la plus simple) :

```bash
kubectl create secret generic iris-secrets --from-literal=api-key=supersecretkey123
```

Inspectez-le ‚Äî notez que la valeur est encod√©e en base64 :

```bash
kubectl get secret iris-secrets -o yaml
```

2. Injectez la cl√© API dans le Deployment via `k8s/server.yaml` :

```yaml
# Ajoutez sous env dans le Deployment :
- name: API_KEY
  valueFrom:
    secretKeyRef:
      name: iris-secrets         # Nom du Secret
      key: api-key               # Cl√© √† injecter
```

3. Appliquez et v√©rifiez :

```bash
kubectl apply -f k8s/server.yaml
kubectl exec -it <nom-du-pod> -- /bin/bash
env | grep API_KEY
```

4. Nettoyez la config et le secret une fois termin√© (on n'en aura plus besoin) :

```bash
kubectl delete configmap iris-config
kubectl delete secret iris-secrets
```

> üí° **Secret vs ConfigMap**
>
> | | ConfigMap | Secret |
> |---|---|---|
> | Donn√©es sensibles | ‚úó | ‚úì |
> | Encodage | Texte brut | Base64 |
> | Usage typique | URLs, noms, param√®tres | Mots de passe, tokens, cl√©s |
>
> En production, on va encore plus loin en utilisant des outils comme **HashiCorp Vault** ou les secrets managers des cloud providers (AWS Secrets Manager, GCP Secret Manager).

---

## 2. Stockage persistant

Les pods sont **√©ph√©m√®res** : quand un pod est supprim√© ou red√©marr√©, son syst√®me de fichiers local est perdu. Pour les bases de donn√©es, on a besoin que les donn√©es survivent aux red√©marrages.

Kubernetes propose les **PersistentVolumes (PV)** et **PersistentVolumeClaims (PVC)** pour g√©rer le stockage persistant. Un PVC est une demande de stockage : vous d√©clarez combien d'espace vous voulez et Kubernetes se charge de le provisionner.

```
Pod  ‚Üí  PersistentVolumeClaim  ‚Üí  PersistentVolume  ‚Üí  Disque physique
        (votre demande)            (la ressource)
```

### üìã Exercice ‚Äî D√©ployer PostgreSQL avec stockage persistant

1. Cr√©ez `k8s/postgres.yaml` :

```yaml
# PVC : on r√©serve 1 Go de stockage pour PostgreSQL
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
spec:
  accessModes:
    - ReadWriteOnce        # Un seul pod peut monter ce volume √† la fois
  resources:
    requests:
      storage: 1Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-db
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15
        env:
        - name: POSTGRES_DB
          value: "irisdb"
        - name: POSTGRES_USER
          value: "iris"
        - name: POSTGRES_PASSWORD
          value: "irispassword"   # En production, utilisez un Secret !
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data   # Donn√©es PostgreSQL dans le conteneur
      volumes:
      - name: postgres-data
        persistentVolumeClaim:
          claimName: postgres-pvc               # R√©f√©rence au PVC ci-dessus
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-svc
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
```

2. Appliquez le manifeste :

```bash
kubectl apply -f k8s/postgres.yaml
kubectl get pods -l app=postgres --watch
# Attendez que le STATUS soit Running, puis Ctrl+C
```

3. V√©rifiez que le PVC est bien li√© √† un volume :

```bash
kubectl get pvc postgres-pvc
```

La colonne `STATUS` doit afficher `Bound`.

4. Connectez-vous √† PostgreSQL et cr√©ez une table de test :

```bash
kubectl exec -it <nom-du-pod-postgres> -- psql -U iris -d irisdb
```

Dans le shell PostgreSQL :

```sql
CREATE TABLE predictions (
  id SERIAL PRIMARY KEY,
  species VARCHAR(50),
  created_at TIMESTAMP DEFAULT NOW()
);

INSERT INTO predictions (species) VALUES ('Iris-setosa');
SELECT * FROM predictions;
\q
```

5. **Testez la persistance** ‚Äî supprimez le pod et attendez qu'il soit recr√©√© par le Deployment :

```bash
kubectl delete pod <nom-du-pod-postgres>
kubectl get pods -l app=postgres --watch
# Attendez que le nouveau pod soit Running, puis Ctrl+C
```

6. Reconnectez-vous et v√©rifiez que les donn√©es sont toujours l√† :

```bash
kubectl exec -it <nouveau-nom-du-pod> -- psql -U iris -d irisdb
SELECT * FROM predictions;
\q
```

Les donn√©es survivent au red√©marrage du pod gr√¢ce au PVC. üéâ

7. Nettoyez :

```bash
kubectl delete -f k8s/postgres.yaml
```

> ‚ö†Ô∏è Supprimer le PVC supprime d√©finitivement les donn√©es. En production, les PVC sont souvent prot√©g√©s par une `reclaimPolicy: Retain`.

---

## 3. CronJobs

Un **CronJob** Kubernetes ex√©cute un Job selon un planning d√©fini, exactement comme un `cron` Unix. C'est id√©al pour les t√¢ches batch : ingestion de donn√©es, r√©entra√Ænement de mod√®les, rapports automatiques.

> üìù **Pr√©requis**
>
> Le backend Iris doit √™tre actif pour cet exercice. Si vous l'avez supprim√©, relancez-le :
>
> ```bash
> kubectl apply -f k8s/server.yaml
> ```

### üìã Exercice ‚Äî Simuler un r√©entra√Ænement automatique

Nous allons cr√©er un CronJob qui appelle l'endpoint `/predict` du backend Iris toutes les minutes ‚Äî pour simuler une t√¢che automatique planifi√©e.

1. Cr√©ez `k8s/cronjob.yaml` :

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: iris-retrainer
spec:
  schedule: "*/1 * * * *"        # Toutes les minutes
  concurrencyPolicy: Forbid       # Ne pas d√©marrer un nouveau Job si le pr√©c√©dent tourne encore
  successfulJobsHistoryLimit: 3   # Conserver les logs des 3 derniers Jobs r√©ussis
  failedJobsHistoryLimit: 1       # Conserver les logs du dernier Job √©chou√©
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: retrainer
            image: curlimages/curl   # Image l√©g√®re qui contient curl
            args:
            - /bin/sh
            - -c
            - |
              curl -X POST http://mlops-server-svc:8000/predict \
                -H 'Content-Type: application/json' \
                -d '{"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}' \
                && echo "Pr√©diction effectu√©e avec succ√®s"
```

2. Appliquez-le :

```bash
kubectl apply -f k8s/cronjob.yaml
```

3. Observez les Jobs cr√©√©s automatiquement (attendez une minute) :

```bash
kubectl get jobs --watch
```

4. Consultez les logs du Job pour voir la r√©ponse de l'API :

```bash
# Remplacez <nom-du-job> par le nom affich√© dans kubectl get jobs
kubectl logs job/<nom-du-job>
```

Vous devriez voir la r√©ponse JSON de l'API avec la classe pr√©dite.

5. Inspectez le CronJob :

```bash
kubectl describe cronjob iris-retrainer
```

La section `Events` montre l'historique des d√©clenchements.

6. D√©clenchez manuellement un Job sans attendre le planning :

```bash
kubectl create job iris-retrainer-manual --from=cronjob/iris-retrainer
kubectl logs job/iris-retrainer-manual
```

7. Nettoyez :

```bash
kubectl delete -f k8s/cronjob.yaml
```

> üí° **Job vs CronJob vs Deployment**
>
> | Ressource | Cas d'usage |
> |---|---|
> | **Deployment** | Services long-running (API, serveur web) |
> | **Job** | T√¢che √† ex√©cution unique (migration, transformation) |
> | **CronJob** | T√¢che planifi√©e r√©currente (ingestion, r√©entra√Ænement) |

---

## 4. D√©ployer un stack IA moderne

Dans le module GenAI, vous avez construit un stack IA avec un **serveur MCP** (FastMCP) et un **agent PydanticAI**. Nous allons d√©ployer ce stack sur Kubernetes en utilisant le DNS K8s pour connecter l'agent au serveur.

> üìù **Pr√©requis**
>
> Avoir compl√©t√© le module GenAI et disposer du code du serveur MCP et de l'agent PydanticAI.

### a. D√©ployer le serveur MCP

Le serveur MCP expose des outils que l'agent peut appeler. On le d√©ploie comme n'importe quel service : un Deployment + un Service.

#### üìã Exercice ‚Äî D√©ployer le serveur MCP

1. Buildez l'image du serveur MCP depuis votre code GenAI :

```bash
docker build -t mcp-server:latest ./mcp-server
```

2. Cr√©ez `k8s/mcp-server.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mcp-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mcp-server
  template:
    metadata:
      labels:
        app: mcp-server
    spec:
      containers:
      - name: mcp-server
        image: mcp-server:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8000
        readinessProbe:
          httpGet:
            path: /
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: mcp-server-svc
spec:
  selector:
    app: mcp-server
  ports:
  - port: 8000
    targetPort: 8000
    nodePort: 30900
  type: NodePort
```

3. Appliquez :

```bash
kubectl apply -f k8s/mcp-server.yaml
kubectl get pods -l app=mcp-server --watch
```

---

### b. D√©ployer l'agent

L'agent doit savoir o√π trouver le serveur MCP. Dans Kubernetes, on utilise le nom DNS du Service ‚Äî exactement comme on l'a fait avec le projet Iris.

#### üìã Exercice ‚Äî D√©ployer l'agent PydanticAI

1. Buildez l'image de l'agent :

```bash
docker build -t pydantic-agent:latest ./agent
```

2. Cr√©ez `k8s/agent.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pydantic-agent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pydantic-agent
  template:
    metadata:
      labels:
        app: pydantic-agent
    spec:
      containers:
      - name: pydantic-agent
        image: pydantic-agent:latest
        imagePullPolicy: IfNotPresent
        env:
          # L'agent utilise le nom DNS du Service MCP pour le joindre dans le cluster
        - name: MCP_SERVER_URL
          value: "http://mcp-server-svc:8000/sse"
```

3. Appliquez et v√©rifiez les logs de l'agent pour confirmer qu'il se connecte au serveur MCP :

```bash
kubectl apply -f k8s/agent.yaml
kubectl logs -l app=pydantic-agent --follow
```

---

### c. Challenge ‚Äî Scaler les agents

#### üèÜ Challenge

Maintenant que le stack fonctionne avec 1 agent et 1 serveur MCP :

1. **Scalez l'agent √† 5 r√©pliques** :

```bash
kubectl scale deployment pydantic-agent --replicas=5
```

2. Observez les pods :

```bash
kubectl get pods -l app=pydantic-agent
```

3. Le serveur MCP g√®re-t-il correctement les connexions concurrentes ? Consultez ses logs :

```bash
kubectl logs -l app=mcp-server --follow
```

4. Si le serveur MCP est satur√©, scalez-le √©galement et observez comment le Service r√©partit la charge entre les r√©pliques :

```bash
kubectl scale deployment mcp-server --replicas=3
kubectl get pods -l app=mcp-server
```

5. Nettoyez tout :

```bash
kubectl delete -f k8s/mcp-server.yaml
kubectl delete -f k8s/agent.yaml
```

> üí° C'est le pattern **scale-out horizontal** de Kubernetes : plut√¥t que d'augmenter les ressources d'une machine, on multiplie les instances. Le Service r√©partit automatiquement la charge entre toutes les r√©pliques.

---

## R√©capitulatif des commandes essentielles

| Commande | Description |
|---|---|
| `kubectl get configmap <nom>` | Inspecter un ConfigMap |
| `kubectl describe configmap <nom>` | D√©tails d'un ConfigMap |
| `kubectl create secret generic <nom> --from-literal=<cl√©>=<valeur>` | Cr√©er un Secret |
| `kubectl get secret <nom> -o yaml` | Voir le contenu d'un Secret (encod√©) |
| `kubectl get pvc` | Lister les PersistentVolumeClaims |
| `kubectl get jobs --watch` | Observer les Jobs en temps r√©el |
| `kubectl create job <nom> --from=cronjob/<nom>` | D√©clencher manuellement un CronJob |
| `kubectl logs job/<nom>` | Logs d'un Job |
| `kubectl scale deployment <nom> --replicas=N` | Scaler un Deployment |
| `kubectl logs -l app=<label> --follow` | Suivre les logs de tous les pods d'un label |
