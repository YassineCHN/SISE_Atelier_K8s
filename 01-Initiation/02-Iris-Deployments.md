# Kubernetes ‚Äî Fiche 2 : D√©ploiement d'un projet fullstack et strat√©gies de mise √† jour

**Projet Iris sur Kubernetes ‚Äî Rolling Update, Blue/Green, Canary**

## Objectifs

- D√©ployer un projet fullstack (frontend Streamlit + backend FastAPI) sur Kubernetes
- Comprendre le service discovery par DNS dans un cluster K8s
- Pratiquer les trois grandes strat√©gies de d√©ploiement en production : Rolling Update, Blue/Green, Canary

> üìù **Pr√©requis**
>
> Fiche 1 termin√©e. Kubernetes lanc√© en local (Docker Desktop ou Minikube), `kubectl` configur√©.

> üìù **Structure du projet**
>
> Le projet Iris vous est fourni dans le dossier `iris-project/` :
>
> ```
> iris-project/
> ‚îú‚îÄ‚îÄ client/          # Frontend Streamlit
> ‚îÇ   ‚îú‚îÄ‚îÄ app.py
> ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
> ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
> ‚îî‚îÄ‚îÄ server/          # Backend FastAPI
>     ‚îú‚îÄ‚îÄ main.py
>     ‚îú‚îÄ‚îÄ train.py
>     ‚îú‚îÄ‚îÄ Dockerfile
>     ‚îî‚îÄ‚îÄ requirements.txt
> ```
>
> Le backend expose trois routes : `GET /`, `GET /version` et `POST /predict`. L'URL du backend est configur√©e via la variable d'environnement `API_URL` dans le frontend ‚Äî c'est cette variable que l'on utilisera pour connecter les deux services via le DNS Kubernetes.

---

## 1. De Docker Compose vers Kubernetes

Vous avez d√©j√† travaill√© sur un projet fullstack Iris dans le TD Docker Compose. L'objectif ici est de porter cette architecture sur Kubernetes en rempla√ßant le `docker-compose.yml` par des manifestes YAML K8s.

| Docker Compose | Kubernetes |
|---|---|
| `service` | Deployment + Service |
| `scale: N` | `replicas: N` |
| `networks` | Labels + selectors |
| `environment` | Variables d'environnement dans le pod |

### a. Construire les images Docker

Nous allons construire **3 versions du backend**, chacune entra√Æn√©e avec un algorithme diff√©rent. Ces 3 versions serviront de base pour les strat√©gies de d√©ploiement en partie 2.

Placez-vous dans le dossier `iris-project/` et lancez les builds :

```bash
# Version 0.1.0 ‚Äî Random Forest
docker build --build-arg MODEL_NAME=rf --build-arg VERSION=0.1.0 -t mlops-server:0.1.0 ./server

# Version 0.2.0 ‚Äî SVM
docker build --build-arg MODEL_NAME=svm --build-arg VERSION=0.2.0 -t mlops-server:0.2.0 ./server

# Version 0.3.0 ‚Äî R√©gression Logistique
docker build --build-arg MODEL_NAME=logreg --build-arg VERSION=0.3.0 -t mlops-server:0.3.0 ./server

# Frontend (une seule version)
docker build -t mlops-client:latest ./client
```

V√©rifiez que les 4 images sont bien pr√©sentes :

```bash
docker images | grep mlops
```

> üí° Le `--build-arg` permet de passer des arguments au `Dockerfile` au moment du build. Ici, `MODEL_NAME` d√©termine quel algorithme est entra√Æn√© et embarqu√© dans l'image. Chaque version est ainsi autonome ‚Äî pas besoin de monter un fichier de mod√®le externe.

### b. D√©ployer sur Kubernetes

Cr√©ez un dossier `k8s/` dans `iris-project/`. C'est l√† que vous placerez tous vos manifestes YAML.

#### üìã Exercice ‚Äî Porter l'architecture Iris sur Kubernetes

**1. Backend ‚Äî `k8s/server.yaml`**

Cr√©ez un Deployment de 3 r√©pliques pour `mlops-server:0.1.0` et un Service NodePort. Le Service doit s'appeler `mlops-server-svc` ‚Äî c'est ce nom que le frontend utilisera pour le joindre via le DNS Kubernetes.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlops-server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mlops-server
  template:
    metadata:
      labels:
        app: mlops-server
    spec:
      containers:
      - name: mlops-server
        image: mlops-server:0.1.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8000
        readinessProbe:
          httpGet:
            path: /
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: mlops-server-svc
spec:
  selector:
    app: mlops-server
  ports:
  - port: 8000
    targetPort: 8000
    nodePort: 30800
  type: NodePort
```

**2. Frontend ‚Äî `k8s/client.yaml`**

Cr√©ez un Deployment d'1 r√©plique pour `mlops-client:latest` et un Service NodePort. La variable d'environnement `API_URL` indique au frontend comment joindre le backend ‚Äî on utilise le nom DNS du Service K8s.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlops-client
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mlops-client
  template:
    metadata:
      labels:
        app: mlops-client
    spec:
      containers:
      - name: mlops-client
        image: mlops-client:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8501
        env:
          # Le frontend utilise ce nom DNS pour joindre le backend √† l'int√©rieur du cluster.
          # Kubernetes r√©sout automatiquement "mlops-server-svc" en IP du Service.
        - name: API_URL
          value: "http://mlops-server-svc:8000"
        readinessProbe:
          httpGet:
            path: /
            port: 8501
          initialDelaySeconds: 10
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: mlops-client-svc
spec:
  selector:
    app: mlops-client
  ports:
  - port: 8501
    targetPort: 8501
    nodePort: 30801
  type: NodePort
```

**3. Appliquer les manifestes**

```bash
kubectl apply -f k8s/server.yaml
kubectl apply -f k8s/client.yaml
```

Attendez que tous les pods soient pr√™ts :

```bash
kubectl get pods --watch
# Attendez que tous les STATUS soient Running, puis Ctrl+C
```

Ouvrez le frontend sur **http://localhost:30801** et faites une pr√©diction. Dans la sidebar, la section "Info backend" affiche la version et le mod√®le actuellement actifs ‚Äî vous devriez voir `version: 0.1.0` et `model: rf`.

> üí° **Le DNS Kubernetes en action**
>
> Quand le frontend envoie une requ√™te √† `http://mlops-server-svc:8000`, Kubernetes r√©sout automatiquement ce nom en l'adresse IP du Service backend ‚Äî sans que vous ayez √† conna√Ætre l'IP d'un seul pod. C'est le m√©canisme de **service discovery** natif de K8s, le m√™me principe que Docker Compose mais √† l'√©chelle d'un cluster.

> üìù **Conservez ce d√©ploiement actif** ‚Äî il servira de base pour toute la partie 2.

---

## 2. Strat√©gies de d√©ploiement

Vous avez maintenant un backend Iris en version `0.1.0` (Random Forest) qui tourne avec 3 r√©pliques. Vous disposez aussi des versions `0.2.0` (SVM) et `0.3.0` (Logistic Regression) pr√™tes √† √™tre d√©ploy√©es.

Dans cette partie, nous allons explorer trois strat√©gies pour mettre √† jour ce backend **sans interrompre le service**.

### a. Rolling Update

Le **Rolling Update** est la strat√©gie par d√©faut de Kubernetes. Les pods sont remplac√©s progressivement : Kubernetes cr√©e un nouveau pod avec la nouvelle image, attend qu'il soit pr√™t, puis supprime un ancien pod ‚Äî et ainsi de suite jusqu'√† ce que tous les pods soient mis √† jour.

```
Avant :   [v0.1.0] [v0.1.0] [v0.1.0]
Pendant : [v0.1.0] [v0.1.0] [v0.2.0]  ‚Üê nouveau pod pr√™t, ancien supprim√©
          [v0.1.0] [v0.2.0] [v0.2.0]
Apr√®s :   [v0.2.0] [v0.2.0] [v0.2.0]
```

#### üìã Exercice ‚Äî Rolling Update vers la version 0.2.0

1. Ouvrez un second terminal et observez le d√©ploiement en temps r√©el :

```bash
kubectl get pods -l app=mlops-server --watch
```

2. Dans votre premier terminal, mettez √† jour l'image dans `k8s/server.yaml` :

```yaml
# Changez la ligne image dans le Deployment :
image: mlops-server:0.2.0
```

3. Appliquez la mise √† jour :

```bash
kubectl apply -f k8s/server.yaml
```

4. Observez le second terminal : les pods `v0.1.0` sont remplac√©s un par un par des pods `v0.2.0`. Le service reste disponible pendant toute la dur√©e de la mise √† jour.

5. V√©rifiez le statut du rollout :

```bash
kubectl rollout status deployment/mlops-server
```

6. Rechargez le frontend sur **http://localhost:30801** et observez la section "Info backend" ‚Äî la version est maintenant `0.2.0` et le mod√®le est `svm`.

7. Simulez un probl√®me et effectuez un **rollback** :

```bash
# Annule la derni√®re mise √† jour et revient √† la version pr√©c√©dente
kubectl rollout undo deployment/mlops-server
```

V√©rifiez que les pods sont revenus en `0.1.0` :

```bash
kubectl get pods -l app=mlops-server
kubectl rollout status deployment/mlops-server
```

> üí° **Quand utiliser le Rolling Update ?**
>
> C'est la strat√©gie recommand√©e par d√©faut. Elle garantit une disponibilit√© continue et permet un rollback rapide. Elle est adapt√©e quand les versions v1 et v2 peuvent coexister sans probl√®me (m√™me format de r√©ponse API, m√™me sch√©ma de base de donn√©es, etc.).

---

### b. Blue/Green Deployment

Le **Blue/Green** consiste √† faire tourner simultan√©ment deux environnements complets : l'environnement actuel (**blue**, v0.1.0) et le nouvel environnement (**green**, v0.3.0). Le basculement du trafic se fait en une seule op√©ration en modifiant le selector du Service ‚Äî z√©ro downtime, et rollback instantan√©.

```
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ   mlops-server-svc  ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ selector: version=blue
                     ‚ñº
Blue  : [v0.1.0] [v0.1.0] [v0.1.0]   ‚Üê trafic actuel
Green : [v0.3.0] [v0.3.0] [v0.3.0]   ‚Üê pr√™t mais sans trafic

         Apr√®s basculement :
                     ‚îÇ selector: version=green
                     ‚ñº
Blue  : [v0.1.0] [v0.1.0] [v0.1.0]   ‚Üê plus de trafic (rollback possible)
Green : [v0.3.0] [v0.3.0] [v0.3.0]   ‚Üê trafic bascul√©
```

#### üìã Exercice ‚Äî Blue/Green vers la version 0.3.0

1. Supprimez le d√©ploiement pr√©c√©dent :

```bash
kubectl delete -f k8s/server.yaml
```

2. Cr√©ez `k8s/server-bluegreen.yaml` avec les deux Deployments et un Service pointant initialement sur blue :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlops-server-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mlops-server
      version: blue
  template:
    metadata:
      labels:
        app: mlops-server
        version: blue
    spec:
      containers:
      - name: mlops-server
        image: mlops-server:0.1.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8000
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlops-server-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mlops-server
      version: green
  template:
    metadata:
      labels:
        app: mlops-server
        version: green
    spec:
      containers:
      - name: mlops-server
        image: mlops-server:0.3.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8000
---
apiVersion: v1
kind: Service
metadata:
  name: mlops-server-svc
spec:
  selector:
    app: mlops-server
    version: blue        # Seuls les pods blue re√ßoivent du trafic
  ports:
  - port: 8000
    targetPort: 8000
    nodePort: 30800
  type: NodePort
```

3. Appliquez le manifeste :

```bash
kubectl apply -f k8s/server-bluegreen.yaml
```

V√©rifiez que les 6 pods (3 blue + 3 green) sont Running :

```bash
kubectl get pods -l app=mlops-server
```

4. V√©rifiez dans le frontend que le backend r√©pond encore en version `0.1.0` (blue).

5. **Basculez le trafic vers green** en modifiant le selector du Service dans `k8s/server-bluegreen.yaml` :

```yaml
# Dans la section spec du Service, changez :
  selector:
    app: mlops-server
    version: green       # Basculement vers green
```

Appliquez la modification :

```bash
kubectl apply -f k8s/server-bluegreen.yaml
```

6. Rechargez le frontend ‚Äî la version est maintenant `0.3.0` (logreg). Le basculement a √©t√© instantan√©.

7. **Rollback instantan√©** : pour revenir √† blue, remettez `version: blue` dans le selector et r√©appliquez. Aucun pod n'a besoin d'√™tre recr√©√©.

> üí° **Quand utiliser le Blue/Green ?**
>
> Quand vous avez besoin d'un basculement instantan√© et d'un rollback imm√©diat. Inconv√©nient : vous devez maintenir le double des ressources pendant la transition (6 pods au lieu de 3).

---

### c. Canary Deployment

Le **Canary** consiste √† exposer la nouvelle version √† une **fraction seulement du trafic**, en faisant tourner un petit nombre de pods `v0.2.0` aux c√¥t√©s des pods `v0.1.0`. Si tout va bien, on augmente progressivement la proportion jusqu'√† migration compl√®te.

```
mlops-server-svc (selector: app=mlops-server)
        ‚îÇ
        ‚îú‚îÄ‚îÄ [v0.1.0] [v0.1.0] [v0.1.0]   ‚Üí 75% du trafic (stable, 3 pods)
        ‚îî‚îÄ‚îÄ [v0.2.0]                       ‚Üí 25% du trafic (canary, 1 pod)
```

Le Service s√©lectionne **tous** les pods avec le label `app=mlops-server`, sans distinguer la version. Kubernetes r√©partit le trafic proportionnellement au nombre de pods.

#### üìã Exercice ‚Äî Canary vers la version 0.2.0

1. Nettoyez les ressources blue/green :

```bash
kubectl delete -f k8s/server-bluegreen.yaml
```

2. Cr√©ez `k8s/server-canary.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlops-server-stable
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mlops-server
      track: stable
  template:
    metadata:
      labels:
        app: mlops-server
        track: stable
    spec:
      containers:
      - name: mlops-server
        image: mlops-server:0.1.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8000
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlops-server-canary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mlops-server
      track: canary
  template:
    metadata:
      labels:
        app: mlops-server
        track: canary
    spec:
      containers:
      - name: mlops-server
        image: mlops-server:0.2.0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8000
---
# Le Service s√©lectionne TOUS les pods avec app=mlops-server,
# sans filtrer sur "track". Le trafic est r√©parti proportionnellement au nombre de pods.
apiVersion: v1
kind: Service
metadata:
  name: mlops-server-svc
spec:
  selector:
    app: mlops-server
  ports:
  - port: 8000
    targetPort: 8000
    nodePort: 30800
  type: NodePort
```

3. Appliquez :

```bash
kubectl apply -f k8s/server-canary.yaml
```

V√©rifiez les 4 pods actifs (3 stable + 1 canary) :

```bash
kubectl get pods -l app=mlops-server
```

4. Dans le frontend, faites plusieurs pr√©dictions successives et observez la section "Info backend" ‚Äî environ 1 requ√™te sur 4 sera servie par la version `0.2.0` (svm).

5. Si le canary est satisfaisant, **migrez progressivement** en ajustant les r√©pliques dans `k8s/server-canary.yaml` puis en r√©appliquant :

```yaml
# √âtape interm√©diaire : 50/50
# mlops-server-stable: replicas: 2
# mlops-server-canary: replicas: 2

# Migration compl√®te
# mlops-server-stable: replicas: 0
# mlops-server-canary: replicas: 4
```

6. En cas de probl√®me, **rollback imm√©diat** en scalant le canary √† 0 :

```bash
kubectl scale deployment mlops-server-canary --replicas=0
```

7. Nettoyez tout une fois termin√© :

```bash
kubectl delete -f k8s/server-canary.yaml
kubectl delete -f k8s/client.yaml
```

> üí° **Quand utiliser le Canary ?**
>
> Quand vous voulez tester une nouvelle version sur une fraction du trafic r√©el avant de d√©ployer massivement. C'est la strat√©gie la plus prudente, particuli√®rement utile pour les mod√®les ML dont les performances r√©elles peuvent diff√©rer des m√©triques d'entra√Ænement.

---

## R√©capitulatif des strat√©gies

| Strat√©gie | Basculement | Rollback | Ressources | Id√©al pour |
|---|---|---|---|---|
| **Rolling Update** | Progressif automatique | `rollout undo` | √ó1 | Mises √† jour courantes |
| **Blue/Green** | Instantan√© (selector) | Modifier selector | √ó2 | Z√©ro downtime garanti |
| **Canary** | Progressif et contr√¥l√© | `scale --replicas=0` | √ó1.x | Validation sur trafic r√©el |

## R√©capitulatif des commandes essentielles

| Commande | Description |
|---|---|
| `kubectl apply -f fichier.yaml` | Cr√©er ou mettre √† jour une ressource |
| `kubectl delete -f fichier.yaml` | Supprimer les ressources d√©clar√©es |
| `kubectl get pods -l app=mlops-server` | Filtrer les pods par label |
| `kubectl rollout status deployment/<nom>` | Suivre l'avancement d'un rollout |
| `kubectl rollout undo deployment/<nom>` | Rollback vers la version pr√©c√©dente |
| `kubectl scale deployment <nom> --replicas=N` | Modifier le nombre de r√©pliques |
