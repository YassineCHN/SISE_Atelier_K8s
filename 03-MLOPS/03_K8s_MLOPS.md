#  Atelier Kubernetes MLOps - Exercices Complets

**Projet :** DÃ©ployer une stack MLOps complÃ¨te pour la prÃ©diction du diabÃ¨te  
**Dataset :** https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv  


---

## Table des matiÃ¨res

- [Exercice 01 : PostgreSQL avec stockage persistant](#exercice-01--postgresql-avec-stockage-persistant)
- [Exercice 02 : MLflow Server](#exercice-02--mlflow-server)
- [Exercice 03 : EntraÃ®ner le modÃ¨le Diabetes](#exercice-03--entraÃ®ner-le-modÃ¨le-diabetes)
- [Exercice 04 : API FastAPI](#exercice-04--api-fastapi)
- [Exercice 05 : Interface Streamlit](#exercice-05--interface-streamlit)
- [Exercice 06 : Auto-scaling avec HPA](#exercice-06--auto-scaling-avec-hpa)

---

# Exercice 01 : PostgreSQL avec stockage persistant

## Objectifs

- Comprendre le stockage persistant dans Kubernetes
- CrÃ©er un PersistentVolumeClaim (PVC)
- DÃ©ployer PostgreSQL avec des donnÃ©es persistantes
- Configurer les variables d'environnement avec des Secrets

##  Concepts clÃ©s

**PersistentVolumeClaim (PVC) :** Demande de stockage persistant. Les donnÃ©es survivent aux redÃ©marrages de pods. Sans PVC, toutes les donnÃ©es de PostgreSQL seraient perdues Ã  chaque redÃ©marrage.

**Secret :** Objet Kubernetes pour stocker des donnÃ©es sensibles (mots de passe, tokens). Les Secrets sont encodÃ©s en base64 et sÃ©parÃ©s du code.

**Service ClusterIP :** Service interne au cluster, non accessible depuis l'extÃ©rieur. Parfait pour les bases de donnÃ©es qu'on ne veut pas exposer.

---

## TÃ¢ches

### TÃ¢che 1.1 : CrÃ©er un Secret pour les credentials PostgreSQL

Les mots de passe ne doivent **jamais** Ãªtre Ã©crits directement dans les fichiers de dÃ©ploiement. On utilise un Secret Kubernetes.

CrÃ©ez un fichier `postgres-secret.yaml` :

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
type: Opaque
stringData:
  POSTGRES_USER: mlflow
  POSTGRES_PASSWORD: mlflow_password
  POSTGRES_DB: mlflow
```

**Questions de comprÃ©hension :**
1. Pourquoi utilise-t-on un Secret plutÃ´t que mettre les mots de passe directement dans le Deployment ?
2. Quelle est la diffÃ©rence entre `data` (valeurs en base64) et `stringData` (valeurs en clair) dans un Secret ?

**Appliquer :**
```bash
kubectl apply -f postgres-secret.yaml
kubectl get secret postgres-secret  # VÃ©rifier que le secret existe
```

---

### TÃ¢che 1.2 : CrÃ©er un PersistentVolumeClaim

Le PVC rÃ©serve de l'espace disque persistant pour PostgreSQL. Sans lui, les donnÃ©es disparaissent Ã  chaque redÃ©marrage du pod.

CrÃ©ez un fichier `postgres-pvc.yaml` :

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
spec:
  accessModes:
    - ReadWriteOnce   # Un seul node peut lire/Ã©crire Ã  la fois
  resources:
    requests:
      storage: 5Gi
```

> ğŸ’¡ **Note :** `ReadWriteOnce` est le mode adaptÃ© pour une base de donnÃ©es sur Minikube. Les autres modes (`ReadOnlyMany`, `ReadWriteMany`) permettent l'accÃ¨s depuis plusieurs nodes simultanÃ©ment.

**Appliquer :**
```bash
kubectl apply -f postgres-pvc.yaml
kubectl get pvc  # VÃ©rifier que le status est "Bound"
```

---

### TÃ¢che 1.3 : CrÃ©er le Deployment PostgreSQL

CrÃ©ez un fichier `postgres-deployment.yaml`.

**Votre mission : ComplÃ©tez les TODO** (les clÃ©s du Secret sont : `POSTGRES_USER`, `POSTGRES_PASSWORD`, `POSTGRES_DB`)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  labels:
    app: postgres
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15
        ports:
        - containerPort: 5432
        env:
        # Injecter les variables depuis le Secret postgres-secret
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: # TODO: mettez le nom du Secret (postgres-secret)
              key:  # TODO: mettez la clÃ© correspondante (POSTGRES_USER)
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: # TODO
              key:  # TODO
        - name: POSTGRES_DB
          valueFrom:
            secretKeyRef:
              name: # TODO
              key:  # TODO
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
          subPath: postgres  # âš ï¸ Important : Ã©vite les erreurs de permissions PostgreSQL
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: postgres-pvc
```

> ğŸ’¡ **Indice :** Le pattern `secretKeyRef` rÃ©fÃ©rence une clÃ© spÃ©cifique dans un Secret. `name` = nom du Secret, `key` = nom de la clÃ© Ã  l'intÃ©rieur du Secret.

**Appliquer :**
```bash
kubectl apply -f postgres-deployment.yaml
kubectl get pods -w  # Attendez que le pod passe en "Running" (Ctrl+C pour quitter)
```

---

### TÃ¢che 1.4 : CrÃ©er le Service PostgreSQL

Le Service permet aux autres pods (MLflow, FastAPI) de trouver PostgreSQL via son nom DNS interne.

CrÃ©ez un fichier `postgres-service.yaml` :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
spec:
  type: ClusterIP  # Accessible uniquement dans le cluster, pas depuis l'extÃ©rieur
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
```

**â“ Questions de comprÃ©hension :**
3. Pourquoi utilise-t-on `ClusterIP` et non `NodePort` pour PostgreSQL ?
4. Comment MLflow pourra-t-il trouver PostgreSQL ? (Indice : Kubernetes crÃ©e automatiquement un DNS `postgres-service` utilisable depuis n'importe quel pod du cluster)

**Appliquer :**
```bash
kubectl apply -f postgres-service.yaml
kubectl get svc
```

---

### TÃ¢che 1.5 : VÃ©rifier le dÃ©ploiement

```bash
# RÃ©cupÃ©rer le nom du pod PostgreSQL
kubectl get pods -l app=postgres

# Se connecter au pod (remplacez <postgres-pod-name> par le vrai nom)
kubectl exec -it <postgres-pod-name> -- psql -U mlflow -d mlflow

# Ou en une seule commande (rÃ©cupÃ¨re automatiquement le nom du pod)
kubectl exec -it $(kubectl get pod -l app=postgres -o jsonpath='{.items[0].metadata.name}') -- psql -U mlflow -d mlflow

# Dans psql :
\l    # Lister les databases
\dt   # Lister les tables (vide pour l'instant, c'est normal)
\q    # Quitter
```

** Questions de validation :**
5. Le PVC est-il en status `Bound` ?
6. Le pod PostgreSQL est-il en status `Running` ?
7. Pouvez-vous vous connecter Ã  la database `mlflow` ?

---

##  Checklist Exercice 01

- [ ] Secret `postgres-secret` crÃ©Ã© et appliquÃ©
- [ ] PVC `postgres-pvc` en status `Bound`
- [ ] Pod PostgreSQL en status `Running`
- [ ] Service `postgres-service` crÃ©Ã© (type ClusterIP)
- [ ] Connexion psql rÃ©ussie

---

# Exercice 02 : MLflow Server

## Objectifs

- Comprendre pourquoi on crÃ©e des images Docker custom
- CrÃ©er une image MLflow avec le driver PostgreSQL
- DÃ©ployer MLflow Tracking Server connectÃ© Ã  PostgreSQL
- Exposer l'UI MLflow via un NodePort

##  Concepts clÃ©s

**MLflow Tracking Server :** Serveur centralisÃ© pour logger les expÃ©riences ML (paramÃ¨tres, mÃ©triques, modÃ¨les). Toute l'Ã©quipe peut y accÃ©der.

**Backend Store :** Base de donnÃ©es oÃ¹ MLflow stocke les mÃ©tadonnÃ©es (expÃ©riences, runs, mÃ©triques, paramÃ¨tres). On utilise PostgreSQL.

**Artifact Store :** Stockage des fichiers binaires (modÃ¨les, images, fichiers). On utilise un volume persistant local.

**Image Docker custom :** L'image officielle MLflow ne contient **pas** les drivers de bases de donnÃ©es. On doit crÃ©er notre propre image.

---

## TÃ¢ches

### TÃ¢che 2.1 : CrÃ©er une image MLflow custom

**Pourquoi ?** L'image officielle `ghcr.io/mlflow/mlflow:v2.17.2` ne contient **pas** le driver PostgreSQL (`psycopg2`). Sans lui, MLflow crashe au dÃ©marrage.

** Ce qui se passe sans psycopg2 :** 
```
ModuleNotFoundError: No module named 'psycopg2'
â†’ Pod en CrashLoopBackOff
```

** Ce qui se passe avec psycopg2 :**
```
[INFO] Listening at: http://0.0.0.0:5000
â†’ MLflow fonctionne !
```

CrÃ©ez un fichier `Dockerfile.mlflow` Ã  la racine de votre dossier de travail :

```dockerfile
FROM ghcr.io/mlflow/mlflow:v2.17.2

# Installer le driver PostgreSQL pour Python
RUN pip install --no-cache-dir psycopg2-binary
```

**â“ Questions de comprÃ©hension :**
1. Pourquoi l'image officielle MLflow n'inclut-elle pas `psycopg2` par dÃ©faut ?
2. Quelle est la diffÃ©rence entre `psycopg2` et `psycopg2-binary` ?

> ğŸ’¡ **RÃ©ponse :** `psycopg2-binary` est une version prÃ©-compilÃ©e qui ne nÃ©cessite pas de compilateur C. Elle est parfaite pour les images Docker.

---

### TÃ¢che 2.2 : CrÃ©er le PVC pour les artefacts MLflow

MLflow a besoin d'un espace persistant pour stocker les modÃ¨les et artefacts.

CrÃ©ez un fichier `mlflow-artifacts-pvc.yaml` :

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mlflow-artifacts-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```

**Appliquer :**
```bash
kubectl apply -f mlflow-artifacts-pvc.yaml
kubectl get pvc  # Vous devez voir postgres-pvc et mlflow-artifacts-pvc en "Bound"
```

---

### TÃ¢che 2.3 : Builder l'image dans Minikube

>  **ATTENTION :** Minikube a son propre daemon Docker isolÃ© de votre Docker local. Si vous buildez l'image dans votre Docker local, Minikube ne la trouvera pas.

```bash
# Ã‰tape 1 : Configurer votre shell pour utiliser le Docker de Minikube si vous utiliser minikube sinon passer directement Ã  l'Ã©tape 2
eval $(minikube docker-env)

#  Sur Windows PowerShell, utilisez plutÃ´t :
# minikube -p minikube docker-env --shell powershell | Invoke-Expression

# Ã‰tape 2 : Builder l'image MLflow custom
docker build -t mlflow-postgres:v1 -f Dockerfile.mlflow .

# Ã‰tape 3 : VÃ©rifier que l'image est bien prÃ©sente dans Minikube
docker images | grep mlflow
```

**RÃ©sultat attendu :**
```
REPOSITORY         TAG   IMAGE ID        CREATED          SIZE
mlflow-postgres    v1    abc123def456    10 seconds ago   ~600MB
```

---

### TÃ¢che 2.4 : CrÃ©er le Deployment MLflow

CrÃ©ez un fichier `mlflow-deployment.yaml`.

**Votre mission : ComplÃ©tez les parties manquantes**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlflow
  labels:
    app: mlflow
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mlflow
  template:
    metadata:
      labels:
        app: mlflow
    spec:
      containers:
      - name: mlflow
        image: # TODO: nom de l'image buildÃ©e Ã  l'Ã©tape prÃ©cÃ©dente (mlflow-postgres:v1)
        imagePullPolicy: Never  # âš ï¸ Obligatoire : cherche l'image localement dans Minikube
        ports:
        - containerPort: 5000
        command:
          - mlflow
          - server
          - --host
          - "0.0.0.0"
          - --port
          - "5000"
          - --backend-store-uri
          # TODO: URI de connexion PostgreSQL
          # Format : postgresql://USER:PASSWORD@SERVICE_NAME:PORT/DB
          # Utilisez postgres-service (nom du Service Kubernetes) comme host
          - # postgresql://mlflow:mlflow_password@???:5432/mlflow
          - --default-artifact-root
          - /mlflow/artifacts
          - --serve-artifacts
        volumeMounts:
        - name: mlflow-artifacts
          mountPath: /mlflow/artifacts
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
      volumes:
      - name: mlflow-artifacts
        persistentVolumeClaim:
          claimName: mlflow-artifacts-pvc
```

> ğŸ’¡ **Indices :**
> - Image : `mlflow-postgres:v1`
> - `imagePullPolicy: Never` est **obligatoire** pour utiliser une image locale dans Minikube
> - URI PostgreSQL : `postgresql://mlflow:mlflow_password@postgres-service:5432/mlflow`
> - `postgres-service` est le nom DNS interne rÃ©solu par Kubernetes (Service Discovery)

**â“ Questions de comprÃ©hension :**
4. Que se passerait-il si on oubliait `imagePullPolicy: Never` ?
5. Pourquoi utilise-t-on `postgres-service` au lieu de l'adresse IP du pod PostgreSQL ?

**Appliquer :**
```bash
kubectl apply -f mlflow-deployment.yaml
kubectl get pods -l app=mlflow -w  # Attendez "Running"
```

---

### TÃ¢che 2.5 : CrÃ©er le Service MLflow

CrÃ©ez un fichier `mlflow-service.yaml` :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mlflow-service
spec:
  type: NodePort  # Accessible depuis l'extÃ©rieur du cluster via Minikube
  selector:
    app: mlflow
  ports:
  - port: 5000
    targetPort: 5000
    nodePort: 30500  # Port fixe pour accÃ©der via Minikube
```

**Appliquer :**
```bash
kubectl apply -f mlflow-service.yaml
```

---

### TÃ¢che 2.6 : VÃ©rifier les logs MLflow

**IMPORTANT :** VÃ©rifiez que MLflow dÃ©marre correctement avant de passer Ã  la suite.

```bash
# Observer les logs en temps rÃ©el
kubectl logs -f $(kubectl get pod -l app=mlflow -o jsonpath='{.items[0].metadata.name}')
```

** Logs de succÃ¨s attendus :**
```
INFO mlflow.store.db.utils: Creating initial MLflow database tables...
INFO mlflow.store.db.utils: Updating database tables
[INFO] Listening at: http://0.0.0.0:5000
```

**âŒ Si vous voyez cette erreur :**
```
ModuleNotFoundError: No module named 'psycopg2'
```
â†’ Causes : Dockerfile.mlflow non crÃ©Ã©, image mal taguÃ©e, ou `imagePullPolicy: Never` oubliÃ©.

**âŒ Si vous voyez :**
```
could not connect to server: Connection refused
```
â†’ PostgreSQL n'est pas encore prÃªt. Attendez et relancez.

---

### TÃ¢che 2.7 : AccÃ©der Ã  l'interface MLflow

```bash
# Option 1 : Port-forward (recommandÃ© pour l'atelier)
kubectl port-forward svc/mlflow-service 5000:5000

# Ouvrez votre navigateur sur : http://localhost:5000

# Option 2 : Via Minikube
minikube service mlflow-service --url
```

---

### TÃ¢che 2.8 : VÃ©rifier la connexion Ã  PostgreSQL

**Dans l'interface MLflow :**
- Vous devez voir la page d'accueil MLflow avec la section "Experiments"
- Aucune erreur affichÃ©e

**VÃ©rification dans PostgreSQL (MLflow doit avoir crÃ©Ã© ses tables) :**
```bash
kubectl exec -it $(kubectl get pod -l app=postgres -o jsonpath='{.items[0].metadata.name}') \
  -- psql -U mlflow -d mlflow -c '\dt'
```

**RÃ©sultat attendu :** une liste de tables comme `experiments`, `runs`, `metrics`, `params`, `tags`, etc.

** Questions de validation :**
6. MLflow a-t-il crÃ©Ã© des tables dans PostgreSQL ?
7. Pouvez-vous accÃ©der Ã  l'interface web MLflow ?
8. Y a-t-il des erreurs dans les logs du pod MLflow ?

---

##  Checklist Exercice 02

- [ ] `Dockerfile.mlflow` crÃ©Ã© avec `ghcr.io/mlflow/mlflow:v2.17.2`
- [ ] PVC `mlflow-artifacts-pvc` en status `Bound`
- [ ] Image `mlflow-postgres:v1` buildÃ©e dans le contexte Minikube
- [ ] Pod MLflow en status `Running` (pas de `CrashLoopBackOff`)
- [ ] Logs MLflow : "Listening at: http://0.0.0.0:5000"
- [ ] Interface web MLflow accessible sur http://localhost:5000
- [ ] Tables MLflow crÃ©Ã©es dans PostgreSQL (`\dt` montre les tables)

---

##  Points clÃ©s Ã  retenir

1. **Images Docker minimales** : Les images officielles ne contiennent que le strict nÃ©cessaire. C'est un choix de sÃ©curitÃ© et de lÃ©gÃ¨retÃ©.
2. **Minikube â‰  Docker local** : `eval $(minikube docker-env)` + `imagePullPolicy: Never` sont indispensables.
3. **Service Discovery** : `postgres-service` est rÃ©solu automatiquement par le DNS interne de Kubernetes. Pas besoin d'IP.
4. **Debugging** : `kubectl logs` et `kubectl describe pod` sont vos meilleurs amis.

---

# Exercice 03 : EntraÃ®ner le modÃ¨le Diabetes

## Objectifs

- Charger et prÃ©parer le dataset Diabetes
- EntraÃ®ner un modÃ¨le Random Forest
- Logger paramÃ¨tres, mÃ©triques et modÃ¨le dans MLflow
- Enregistrer le modÃ¨le et lui assigner l'alias `champion`

## Concept important : Model Aliases MLflow 2.17.x

>  **Breaking change MLflow 2.17.x :** Les stages `Production`, `Staging`, `Archived` sont **dÃ©prÃ©ciÃ©s**. La nouvelle approche recommandÃ©e est d'utiliser des **Model Aliases** (ex: `@champion`, `@challenger`). L'API de chargement change Ã©galement :
>

##  Dataset Diabetes

**URL :** https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv

**8 features :** Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age

**Target :** Outcome (0 = pas de diabÃ¨te, 1 = diabÃ¨te) â€” 768 observations

---

##  TÃ¢ches

### TÃ¢che 3.1 : CrÃ©er le script d'entraÃ®nement

CrÃ©ez un fichier `train.py` dans votre dossier de travail.

**Votre mission : ComplÃ©tez les TODO**

```python
import pandas as pd
import mlflow
import mlflow.sklearn
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score
)
from mlflow import MlflowClient
import os

# â”€â”€â”€ Configuration MLflow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000")
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
mlflow.set_experiment("diabetes-classification")

print(" EntraÃ®nement du modÃ¨le Diabetes")

# â”€â”€â”€ 1. Charger les donnÃ©es â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
url = "https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv"
df = pd.read_csv(url)
print(f"Dataset shape: {df.shape}")
print(f"Colonnes: {list(df.columns)}")

# TODO: PrÃ©parer X (toutes les colonnes sauf 'Outcome') et y (colonne 'Outcome')
X = # ???
y = # ???

# TODO: Train/Test split â€” 80% train, 20% test, stratifiÃ©, random_state=42
X_train, X_test, y_train, y_test = train_test_split(
    # ???
)

print(f"Train: {X_train.shape}, Test: {X_test.shape}")

# â”€â”€â”€ 2. EntraÃ®nement avec MLflow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with mlflow.start_run(run_name="diabetes-rf-model"):

    params = {
        "n_estimators": 100,
        "max_depth": 10,
        "min_samples_split": 5,
        "min_samples_leaf": 2,
        "random_state": 42
    }

    # TODO: CrÃ©er le RandomForestClassifier avec les params et l'entraÃ®ner
    model = RandomForestClassifier(**params)
    # ???

    # PrÃ©dictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    # Calcul des mÃ©triques
    accuracy  = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall    = recall_score(y_test, y_pred)
    f1        = f1_score(y_test, y_pred)
    auc       = roc_auc_score(y_test, y_pred_proba)

    # TODO: Logger tous les paramÃ¨tres via mlflow.log_param()
    # Parcourez le dictionnaire params et loggez chaque clÃ©/valeur
    for param_name, param_value in params.items():
        # mlflow.log_param(???, ???)
        pass

    # TODO: Logger les 5 mÃ©triques via mlflow.log_metric()
    # mlflow.log_metric("accuracy", accuracy)
    # ... (precision, recall, f1_score, auc_roc)

    # TODO: Logger le modÃ¨le avec mlflow.sklearn.log_model()
    # ParamÃ¨tres attendus :
    #   - sk_model    : le modÃ¨le entraÃ®nÃ©
    #   - artifact_path : "model"
    #   - registered_model_name : "diabetes-model"
    #   - input_example : X_test.iloc[:5]  (optionnel mais recommandÃ©)
    # mlflow.sklearn.log_model(???)

    print(f"\n RÃ©sultats:")
    print(f"Accuracy:  {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1-Score:  {f1:.4f}")
    print(f"AUC:       {auc:.4f}")
    print(f"\n ModÃ¨le enregistrÃ© dans MLflow!")

# â”€â”€â”€ 3. Assigner l'alias "champion" au modÃ¨le enregistrÃ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  MLflow 2.17.x : on utilise des aliases Ã  la place des stages dÃ©prÃ©ciÃ©s
client = MlflowClient()

# RÃ©cupÃ©rer la derniÃ¨re version enregistrÃ©e du modÃ¨le "diabetes-model"
# TODO: ComplÃ©tez la rÃ©cupÃ©ration de la derniÃ¨re version
latest_version = client.get_registered_model("diabetes-model").latest_versions
last_v = max([int(v.version) for v in latest_version])

# TODO: Assigner l'alias "champion" Ã  cette version
# client.set_registered_model_alias(???, "champion", str(last_v))

print(f"\n Alias 'champion' assignÃ© Ã  la version {last_v} du modÃ¨le diabetes-model")
print(" PrÃªt pour l'exercice 04 !")
```

> ğŸ’¡ **Indices rÃ©capitulatifs :**
> - `X = df.drop('Outcome', axis=1)` â€” toutes les colonnes sauf la cible
> - `y = df['Outcome']` â€” la colonne cible
> - `train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)`
> - `model.fit(X_train, y_train)`
> - `mlflow.log_param(param_name, param_value)`
> - `mlflow.log_metric("accuracy", accuracy)` â€” faites pareil pour les autres mÃ©triques
> - `mlflow.sklearn.log_model(model, "model", registered_model_name="diabetes-model", input_example=X_test.iloc[:5])`
> - `client.set_registered_model_alias("diabetes-model", "champion", str(last_v))`

---

### TÃ¢che 3.2 : CrÃ©er requirements.txt

CrÃ©ez `requirements.txt` dans le dossier de travail :

```
pandas
scikit-learn
mlflow==2.17.2
numpy
setuptools
fastapi
uvicorn
pydantic
```

---

### TÃ¢che 3.3 : Lancer l'entraÃ®nement

```bash
# crÃ©er l'environnement avec uv

# verifier que uv est installer 
uv --version
# sinon 
pip install uv

# creer l'environnement avec python 3.11 version stable pour ce workshop:
uv venv --python 3.11 
# ou avec nom personnalisÃ©
uv venv myvenv --python 3.11 

# activer 
source .venv/Scripts/activate

# installer
pip install -r requirements.txt

# revenez Ã  la racine du projet avec cd ..

# Terminal 1 : Port-forward MLflow (laissez ce terminal ouvert)
kubectl port-forward svc/mlflow-service 5000:5000

# Terminal 2 : EntraÃ®ner le modÃ¨le
export MLFLOW_TRACKING_URI=http://localhost:5000

# Lancer l'entraÃ®nement
python train.py
```

---

### TÃ¢che 3.4 : VÃ©rifier dans MLflow UI

Ouvrez http://localhost:5000 et vÃ©rifiez :

1. L'expÃ©rience **"diabetes-classification"** existe dans le menu gauche
2. Un run **"diabetes-rf-model"** est visible avec toutes les mÃ©triques
3. Dans **"Models"** â†’ **"diabetes-model"** : la version 1 existe avec l'alias **"champion"** affichÃ©

> ğŸ’¡ **Pour voir l'alias dans l'UI :** Models â†’ diabetes-model â†’ Version 1 â†’ vous devez voir `@champion` dans les alias.

**MÃ©triques attendues :**
- Accuracy : ~0.76â€“0.80
- Precision : ~0.71â€“0.75
- Recall : ~0.60â€“0.68
- F1-Score : ~0.65â€“0.72
- AUC : ~0.82â€“0.86

** Questions de validation :**
9. Le modÃ¨le s'entraÃ®ne-t-il sans erreur ?
10. Les 5 mÃ©triques sont-elles loggÃ©es dans MLflow ?
11. L'alias `champion` est-il bien visible sur le modÃ¨le ?

---

## Checklist Exercice 03

- [ ] Script `train.py` fonctionne sans erreur
- [ ] ExpÃ©rience "diabetes-classification" visible dans MLflow
- [ ] 5 mÃ©triques loggÃ©es (accuracy, precision, recall, f1_score, auc_roc)
- [ ] ModÃ¨le "diabetes-model" visible dans la section Models
- [ ] **Alias `champion` assignÃ© au modÃ¨le** â­ (indispensable pour l'exercice 04)

---

# Exercice 04 : API FastAPI


##  Objectifs

- CrÃ©er une API REST avec FastAPI
- Charger le modÃ¨le depuis MLflow via l'alias `@champion`
- Exposer les endpoints `/health` et `/predict`
- DÃ©ployer l'API sur Kubernetes avec 3 replicas

##  Concepts clÃ©s

**Model Alias `@champion`** : Permet de charger toujours le "meilleur" modÃ¨le sans connaÃ®tre son numÃ©ro de version. Format : `models:/diabetes-model@champion`.

**Livenessprobes / Readinessprobe** : Kubernetes vÃ©rifie rÃ©guliÃ¨rement que le pod fonctionne (`/health`). Si la probe Ã©choue, le pod est redÃ©marrÃ© ou retirÃ© du load balancer.

---

##  TÃ¢ches

### TÃ¢che 4.1 : CrÃ©er l'API FastAPI

CrÃ©ez un fichier `main.py` dans un dossier `fastapi/` (ou Ã  la racine si vous prÃ©fÃ©rez tout au mÃªme endroit).

**Votre mission : ComplÃ©tez les TODO**

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import mlflow
import mlflow.sklearn
import numpy as np
import os

app = FastAPI(
    title="Diabetes Prediction API",
    description="API de prÃ©diction du risque de diabÃ¨te â€” MLflow 2.17.2",
    version="1.0.0"
)

# â”€â”€â”€ Configuration MLflow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000")
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

# â”€â”€â”€ Chargement du modÃ¨le au dÃ©marrage de l'API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  MLflow 2.17.x : on charge via l'alias @champion (plus de stage "Production")
# Format : "models:/NOM_DU_MODELE@ALIAS"
try:
    # TODO: Chargez le modÃ¨le avec mlflow.sklearn.load_model()
    # Utilisez l'URI : "models:/diabetes-model@champion"
    model = # ???
    print(" ModÃ¨le chargÃ© depuis MLflow (alias @champion)")
except Exception as e:
    print(f" Erreur chargement modÃ¨le: {e}")
    model = None

# â”€â”€â”€ SchÃ©mas Pydantic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class PredictionRequest(BaseModel):
    Pregnancies: float
    Glucose: float
    BloodPressure: float
    SkinThickness: float
    Insulin: float
    BMI: float
    DiabetesPedigreeFunction: float
    Age: float

class PredictionResponse(BaseModel):
    has_diabetes: bool
    probability: float
    risk_level: str

# â”€â”€â”€ Endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.get("/")
def root():
    return {
        "message": "Diabetes Prediction API",
        "model_loaded": model is not None,
        "mlflow_uri": MLFLOW_TRACKING_URI,
        "endpoints": {"/health": "GET", "/predict": "POST", "/docs": "GET"}
    }

@app.get("/health")
def health():
    # Cet endpoint est utilisÃ© par les Kubernetes probes
    return {
        "status": "healthy",
        "model_loaded": model is not None
    }

@app.post("/predict", response_model=PredictionResponse)
def predict(request: PredictionRequest):
    if model is None:
        raise HTTPException(status_code=503, detail="ModÃ¨le non chargÃ©")

    # TODO: Construire le tableau numpy avec les features dans le bon ordre
    # L'ordre doit correspondre aux colonnes du dataset original :
    # Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI,
    # DiabetesPedigreeFunction, Age
    features = np.array([[
        # ???  (8 valeurs dans l'ordre ci-dessus)
    ]])

    # TODO: Faire la prÃ©diction
    prediction = # model.predict(???)
    proba      = # model.predict_proba(???)

    has_diabetes = bool(prediction[0] == 1)
    probability  = float(proba[0][1])  # ProbabilitÃ© de la classe 1 (diabÃ¨te)

    # Niveau de risque basÃ© sur la probabilitÃ©
    if probability < 0.3:
        risk_level = "Faible"
    elif probability < 0.7:
        risk_level = "ModÃ©rÃ©"
    else:
        risk_level = "Ã‰levÃ©"

    return PredictionResponse(
        has_diabetes=has_diabetes,
        probability=probability,
        risk_level=risk_level
    )
```

> ğŸ’¡ **Indices :**
> - Chargement : `mlflow.sklearn.load_model("models:/diabetes-model@champion")`
> - Features : `request.Pregnancies, request.Glucose, request.BloodPressure, request.SkinThickness, request.Insulin, request.BMI, request.DiabetesPedigreeFunction, request.Age`
> - PrÃ©diction : `model.predict(features)` et `model.predict_proba(features)`

---

### TÃ¢che 4.2 : CrÃ©er le Dockerfile

CrÃ©ez `Dockerfile` :

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY main.py .

# Variable d'environnement par dÃ©faut (sera surchargÃ©e dans le deployment Kubernetes)
ENV MLFLOW_TRACKING_URI=http://mlflow-service:5000

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

### TÃ¢che 4.4 : Builder l'image

```bash
# S'assurer qu'on est dans le contexte Minikube si on utilise minikube sinon on passe directement au build
eval $(minikube docker-env)

# Builder l'image
docker build -t diabetes-api:v1 .

# VÃ©rifier
docker images | grep diabetes-api
```

---

### TÃ¢che 4.5 : CrÃ©er le Deployment Kubernetes

CrÃ©ez `fastapi-deployment.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-api
  labels:
    app: ml-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-api
  template:
    metadata:
      labels:
        app: ml-api
    spec:
      containers:
      - name: api
        image: diabetes-api:v1
        imagePullPolicy: Never
        ports:
        - containerPort: 8000
        env:
        - name: MLFLOW_TRACKING_URI
          value: "http://mlflow-service:5000"  # Service Discovery Kubernetes
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
          limits:
            cpu: 500m
            memory: 1Gi
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 90   # Laisser le temps de charger le modÃ¨le depuis MLflow
          periodSeconds: 15
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
```

> ğŸ’¡ **Note :** Les `initialDelaySeconds` sont intentionnellement gÃ©nÃ©rÃ©s. Le chargement du modÃ¨le depuis MLflow peut prendre 20-30 secondes au premier dÃ©marrage.

---

### TÃ¢che 4.6 : CrÃ©er le Service FastAPI

CrÃ©ez `fastapi-service.yaml` :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: ml-api-service
spec:
  type: NodePort
  selector:
    app: ml-api
  ports:
  - port: 8000
    targetPort: 8000
    nodePort: 30800
```

---

### TÃ¢che 4.7 : DÃ©ployer et tester

```bash
# DÃ©ployer
kubectl apply -f fastapi-deployment.yaml
kubectl apply -f fastapi-service.yaml

# VÃ©rifier que les 3 pods dÃ©marrent (peut prendre 1-2 min)
kubectl get pods -l app=ml-api -w

# Observer les logs (cherchez " ModÃ¨le chargÃ© depuis MLflow")
kubectl logs -l app=ml-api --prefix

# Port-forward pour tester
kubectl port-forward svc/ml-api-service 8000:8000
```

**Tester avec curl :**
```bash
# Health check
curl http://localhost:8000/health
# Attendu : {"status":"healthy","model_loaded":true}

# PrÃ©diction (patient Ã  risque Ã©levÃ©)
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "Pregnancies": 6,
    "Glucose": 148,
    "BloodPressure": 72,
    "SkinThickness": 35,
    "Insulin": 0,
    "BMI": 33.6,
    "DiabetesPedigreeFunction": 0.627,
    "Age": 50
  }'
```

**RÃ©sultat attendu :**
```json
{
  "has_diabetes": true,
  "probability": 0.82,
  "risk_level": "Ã‰levÃ©"
}
```

> ğŸ’¡ **Si `model_loaded: false` :** VÃ©rifiez les logs du pod. L'URI MLflow est-elle correcte ? Le modÃ¨le a-t-il bien l'alias `@champion` ?

**Questions de validation :**
12. `model_loaded` est-il `true` dans `/health` ?
13. Les 3 replicas sont-ils en status `Running` ?
14. L'endpoint `/predict` renvoie-t-il un rÃ©sultat cohÃ©rent ?

---

##  Checklist Exercice 04

- [ ] `main.py` crÃ©Ã© avec chargement via `models:/diabetes-model@champion`
- [ ] Image Docker `diabetes-api:v1` buildÃ©e dans Minikube
- [ ] 3 replicas en status `Running`
- [ ] `/health` rÃ©pond avec `{"status":"healthy","model_loaded":true}`
- [ ] `/predict` renvoie une prÃ©diction correcte
- [ ] Documentation accessible sur http://localhost:8000/docs

---

# Exercice 05 : Interface Streamlit


## Objectifs

- CrÃ©er une interface utilisateur avec Streamlit
- Connecter Streamlit Ã  l'API FastAPI via HTTP
- DÃ©ployer sur Kubernetes avec 2 replicas

---

## TÃ¢ches

### TÃ¢che 5.1 : CrÃ©er l'application Streamlit

CrÃ©ez un fichier `app.py` dans un dossier `streamlit/`.

**Votre mission : ComplÃ©tez les TODO**

```python
import streamlit as st
import requests
import os

# Configuration
API_URL = os.getenv("API_URL", "http://localhost:8000")

st.set_page_config(
    page_title=" Diabetes Predictor",
    page_icon="ğŸ©º",
    layout="wide"
)

st.title(" Diabetes Risk Predictor")
st.markdown("### PrÃ©diction du risque de diabÃ¨te avec Machine Learning")

# Sidebar
with st.sidebar:
    st.header(" Ã€ propos")
    st.markdown("""
    Cette application utilise un modÃ¨le **Random Forest**
    pour prÃ©dire le risque de diabÃ¨te.

    **Stack MLOps :**
    -  Scikit-learn
    -  MLflow 2.17.2
    -  FastAPI
    -  Streamlit
    -  Kubernetes
    """)

    # VÃ©rification du statut de l'API
    try:
        response = requests.get(f"{API_URL}/health", timeout=2)
        if response.status_code == 200:
            data = response.json()
            if data.get("model_loaded"):
                st.success(" API connectÃ©e â€” modÃ¨le chargÃ©")
            else:
                st.warning(" API connectÃ©e â€” modÃ¨le non chargÃ©")
        else:
            st.error(" API indisponible")
    except Exception:
        st.error(" API injoignable")

# Layout principal
col1, col2 = st.columns(2)

with col1:
    st.subheader(" Informations patient")
    pregnancies    = st.slider("Grossesses", 0, 17, 3)
    glucose        = st.slider("Glucose (mg/dL)", 0, 200, 120)
    blood_pressure = st.slider("Pression artÃ©rielle (mm Hg)", 0, 122, 70)
    skin_thickness = st.slider("Ã‰paisseur peau (mm)", 0, 99, 20)
    insulin        = st.slider("Insuline (mu U/ml)", 0, 846, 79)
    bmi            = st.slider("IMC", 0.0, 67.0, 32.0, 0.1)
    dpf            = st.slider("Diabetes Pedigree Function", 0.0, 2.5, 0.5, 0.01)
    age            = st.slider("Ã‚ge", 21, 81, 33)

with col2:
    st.subheader(" RÃ©sultat")

    if st.button(" PrÃ©dire", type="primary", use_container_width=True):
        with st.spinner("Analyse en cours..."):
            try:
                # TODO: Construire le payload (dictionnaire) avec toutes les features
                # Les clÃ©s doivent correspondre exactement aux champs de PredictionRequest :
                # Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin,
                # BMI, DiabetesPedigreeFunction, Age
                payload = {
                    # ???
                }

                # TODO: Appeler l'endpoint POST /predict de l'API
                response = requests.post(
                    f"{API_URL}/predict",
                    json=payload,
                    timeout=5
                )

                if response.status_code == 200:
                    result = response.json()

                    # Affichage du rÃ©sultat
                    if result["has_diabetes"]:
                        st.error(" Risque de diabÃ¨te dÃ©tectÃ©")
                    else:
                        st.success(" Pas de diabÃ¨te dÃ©tectÃ©")

                    # MÃ©triques
                    col_a, col_b = st.columns(2)
                    with col_a:
                        st.metric("ProbabilitÃ©", f"{result['probability']*100:.1f}%")
                    with col_b:
                        st.metric("Niveau de risque", result['risk_level'])

                    st.progress(result['probability'])
                else:
                    st.error(f"Erreur API : {response.status_code}")

            except Exception as e:
                st.error(f" Erreur : {str(e)}")

st.markdown("---")
st.markdown("ğŸ“ Master 2 SISE - Atelier Kubernetes & MLOps")
```

> ğŸ’¡ **Indice pour le payload :**
> ```python
> payload = {
>     "Pregnancies": pregnancies,
>     "Glucose": glucose,
>     "BloodPressure": blood_pressure,
>     "SkinThickness": skin_thickness,
>     "Insulin": insulin,
>     "BMI": bmi,
>     "DiabetesPedigreeFunction": dpf,
>     "Age": age
> }
> ```

---

### TÃ¢che 5.2 : CrÃ©er requirements.txt

```
streamlit==1.29.0
requests==2.31.0
```

---

### TÃ¢che 5.3 : CrÃ©er le Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY app.py .

ENV API_URL=http://ml-api-service:8000

EXPOSE 8501

CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

---

### TÃ¢che 5.4 : Builder et dÃ©ployer

```bash
# Builder dans le contexte Minikube
eval $(minikube docker-env)
docker build -t diabetes-streamlit:v1 .
```

**CrÃ©ez `streamlit-deployment.yaml` :**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: streamlit
spec:
  replicas: 2
  selector:
    matchLabels:
      app: streamlit
  template:
    metadata:
      labels:
        app: streamlit
    spec:
      containers:
      - name: streamlit
        image: diabetes-streamlit:v1
        imagePullPolicy: Never
        ports:
        - containerPort: 8501
        env:
        - name: API_URL
          value: "http://ml-api-service:8000"
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
```

**CrÃ©ez `streamlit-service.yaml` :**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: streamlit-service
spec:
  type: NodePort
  selector:
    app: streamlit
  ports:
  - port: 8501
    targetPort: 8501
    nodePort: 30851
```

**DÃ©ployer :**
```bash
kubectl apply -f streamlit-deployment.yaml
kubectl apply -f streamlit-service.yaml

# AccÃ©der Ã  l'interface
minikube service streamlit-service --url
```

**Questions de validation :**
15. L'interface Streamlit s'affiche-t-elle dans le navigateur ?
16. La sidebar indique-t-elle " API connectÃ©e â€” modÃ¨le chargÃ©" ?
17. Pouvez-vous faire une prÃ©diction avec les sliders ?

---

##  Checklist Exercice 05

- [ ] App Streamlit crÃ©Ã©e avec payload correct
- [ ] Image Docker `diabetes-streamlit:v1` buildÃ©e dans Minikube
- [ ] 2 replicas en status `Running`
- [ ] Interface accessible via `minikube service streamlit-service --url`
- [ ] PrÃ©dictions fonctionnent
- [ ] Indicateur API vert dans la sidebar

---

# Exercice 06 : Auto-scaling avec HPA


##  Objectifs

- Activer le Metrics Server sur Minikube
- Configurer un HorizontalPodAutoscaler (HPA)
- Observer le scaling automatique sous charge

##  Concepts clÃ©s

**HPA (HorizontalPodAutoscaler) :** Ajuste automatiquement le nombre de replicas d'un Deployment selon des mÃ©triques (CPU, RAM, mÃ©triques custom).

**Metrics Server :** Composant Kubernetes qui collecte les mÃ©triques de ressources (CPU/RAM) des pods et nodes. NÃ©cessaire pour que le HPA fonctionne.

---

## TÃ¢ches

### TÃ¢che 6.1 : Activer et vÃ©rifier Metrics Server

```bash
# Activer l'addon metrics-server dans Minikube
minikube addons enable metrics-server

# VÃ©rifier qu'il est opÃ©rationnel (peut prendre 1-2 minutes)
kubectl top nodes
kubectl top pods
```

> Si vous obtenez `error: metrics not available` : attendez 1-2 minutes le temps que metrics-server collecte ses premiÃ¨res mÃ©triques.

---

### TÃ¢che 6.2 : CrÃ©er le HPA

CrÃ©ez `hpa.yaml` :

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-api          # Cible notre dÃ©ploiement FastAPI
  minReplicas: 2          # Jamais moins de 2 replicas
  maxReplicas: 10         # Jamais plus de 10 replicas
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50   # Scale up si CPU > 50%
```

**Questions de comprÃ©hension :**
18. Que se passe-t-il exactement quand l'utilisation CPU dÃ©passe 50% ?
19. Pourquoi est-il important de fixer un `maxReplicas` ?

**Appliquer et observer :**
```bash
kubectl apply -f hpa.yaml

# VÃ©rifier l'Ã©tat du HPA (attendez que TARGETS affiche des valeurs)
kubectl get hpa ml-api-hpa -w
```

---

### TÃ¢che 6.3 : GÃ©nÃ©rer de la charge

**Dans un terminal : observer le HPA**
```bash
kubectl get hpa -w
```

**Dans un autre terminal : gÃ©nÃ©rer la charge**

Option simple â€” load generator avec busybox :
```bash
kubectl run load-generator \
  --rm -it --image=busybox --restart=Never \
  -- /bin/sh -c "while true; do wget -q -O- http://ml-api-service:8000/health; done"
```

Option avancÃ©e â€” prÃ©dictions en masse :
```bash
kubectl port-forward svc/ml-api-service 8000:8000 &

for i in {1..200}; do
  curl -s -X POST http://localhost:8000/predict \
    -H "Content-Type: application/json" \
    -d '{"Pregnancies":6,"Glucose":148,"BloodPressure":72,"SkinThickness":35,"Insulin":0,"BMI":33.6,"DiabetesPedigreeFunction":0.627,"Age":50}' &
done
```

---

### TÃ¢che 6.4 : Observer le scaling

```bash
# Terminal 1 : Watch HPA
kubectl get hpa -w

# Terminal 2 : Watch les pods
kubectl get pods -l app=ml-api -w

# Terminal 3 : Events HPA
kubectl get events --sort-by='.lastTimestamp' | grep HorizontalPodAutoscaler
```

**Comportement attendu :**

1. **MontÃ©e en charge :**
   - CPU dÃ©passe 50% : `TARGETS: 75%/50%`
   - HPA crÃ©e de nouveaux pods : `REPLICAS: 2 â†’ 4 â†’ 6`

2. **Stabilisation :**
   - La charge est distribuÃ©e sur plus de pods
   - CPU redescend : `TARGETS: 40%/50%`

3. **Scale-down (aprÃ¨s ~5 min de calme) :**
   - Pods en surplus supprimÃ©s
   - Retour Ã  `minReplicas: 2`

**Questions de validation :**
20. Les replicas augmentent-ils quand la charge monte ?
21. Combien de temps s'Ã©coule entre le dÃ©passement du seuil et la crÃ©ation des nouveaux pods ?
22. Pourquoi le scale-down est-il plus lent que le scale-up ? (Indice : stabilitÃ©)

---

## Checklist Exercice 06

- [ ] Metrics Server activÃ© (`kubectl top pods` fonctionne)
- [ ] HPA crÃ©Ã© et `TARGETS` affiche des valeurs
- [ ] Sous charge, `REPLICAS` augmente au-delÃ  de 2
- [ ] CPU utilization visible dans `kubectl get hpa`
- [ ] Scale-down fonctionne aprÃ¨s rÃ©duction de charge

---

## ğŸ‰ FÃ©licitations !

**Vous avez terminÃ© tous les exercices !**

Vous avez maintenant une **stack MLOps complÃ¨te en production sur Kubernetes** :

âœ… PostgreSQL avec stockage persistant  
âœ… MLflow 2.17.2 avec backend PostgreSQL et alias `@champion`  
âœ… ModÃ¨le Random Forest entraÃ®nÃ© et accessible  
âœ… API FastAPI scalable (3 replicas)  
âœ… Interface Streamlit (2 replicas)  
âœ… Auto-scaling HPA configurÃ©

---

## Validation Finale

```bash
# Vue d'ensemble de tous les objets Kubernetes
kubectl get all
kubectl get pvc
kubectl get hpa

# Status des pods
kubectl get pods
kubectl top pods

# AccÃ©der Ã  l'application
minikube service streamlit-service --url

# Tester l'API directement
kubectl port-forward svc/ml-api-service 8000:8000
curl http://localhost:8000/health
```

**Tous les pods doivent Ãªtre en `Running` !**

---

## Ce que vous maÃ®trisez maintenant

-  DÃ©ploiement d'applications multi-composants sur Kubernetes
-  Gestion du stockage persistant (PVC)
-  Service Discovery et networking interne
-  Images Docker custom (MLflow + psycopg2)
-  Configuration avec Secrets et Variables d'environnement
-  Health checks (liveness/readiness probes)
-  Auto-scaling horizontal (HPA)
-  Stack MLOps complÃ¨te (PostgreSQL + MLflow 2.17.2 + FastAPI + Streamlit)
-  Model Aliases MLflow (nouvelle approche vs stages dÃ©prÃ©ciÃ©s)
-  Debugging avec `kubectl logs`, `kubectl describe`, `kubectl get events`

---

## Pour aller plus loin

1. **Namespaces** : Isoler la stack dans un namespace `mlops-workshop`
2. **Ingress** : Un seul point d'entrÃ©e HTTP pour tous les services
3. **Monitoring** : Ajouter Prometheus + Grafana
4. **CI/CD** : GitHub Actions pour rebuild et redÃ©ploiement automatique
5. **Sealed Secrets** : Chiffrement des secrets en production
6. **CronJob** : Backup automatique PostgreSQL
