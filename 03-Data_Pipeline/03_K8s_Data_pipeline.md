# Kubernetes ‚Äî Pipeline de Donn√©es

Dans ce TD, nous allons construire un **pipeline de donn√©es batch complet sur Kubernetes** de A √† Z.

En partant de donn√©es m√©t√©o brutes r√©cup√©r√©es depuis une API publique, nous allons les stocker dans un objet-store, les transformer avec dbt, puis exposer les r√©sultats via une API FastAPI ‚Äî le tout orchestr√© par Kubernetes.

```
[API Open-Meteo]
      ‚îÇ
      ‚ñº
[CronJob K8s]  ‚îÄ‚îÄ‚îÄ‚îÄ ingestion des donn√©es brutes ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  [MinIO : raw/]
                                                                ‚îÇ
                                                       [Job K8s : dbt]
                                                                ‚îÇ transformation
                                                                ‚ñº
                                                    [MinIO : transformed/]
                                                                ‚îÇ
                                                       [Service FastAPI]
                                                                ‚îÇ
                                                                ‚ñº
                                                     GET /weather/summary
```

√Ä la fin de ce TD, vous aurez vu comment Kubernetes peut orchestrer un vrai stack data engineering avec des outils largement utilis√©s en entreprise : **MinIO**, **dbt** et **FastAPI**.

---

## Pr√©requis

- Kubernetes lanc√© en local (Docker Desktop avec Kubernetes activ√©, ou Minikube)
- `kubectl` configur√© et connect√© √† votre cluster
- `docker` CLI disponible
- Notions de base sur les Pods, Deployments, Services et PersistentVolumeClaims (vues dans le TD principal Kubernetes)

!!! note "Docker Desktop vs Minikube"
    Tous les manifestes YAML de ce TD fonctionnent de fa√ßon identique sur les deux environnements. Les seules diff√©rences sont :

    - **Acc√®s aux services** : avec Docker Desktop, les services NodePort sont directement accessibles sur `localhost`. Avec Minikube, utilisez `minikube service <nom-du-service>` pour ouvrir un service, ou `minikube tunnel` pour exposer les services LoadBalancer.
    - **Images Docker locales** : avec Docker Desktop, les images build√©es localement sont imm√©diatement disponibles pour Kubernetes. Avec Minikube, buildez les images dans l'environnement Docker de Minikube en ex√©cutant d'abord `eval $(minikube docker-env)`, puis `docker build`.

!!! note "Structure du projet"
    Cr√©ez un r√©pertoire de travail pour ce TD et organisez-le comme suit :

    ```
    k8s-data-pipeline/
    ‚îú‚îÄ‚îÄ ingester/
    ‚îÇ   ‚îú‚îÄ‚îÄ ingest.py
    ‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
    ‚îú‚îÄ‚îÄ transformer/
    ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
    ‚îÇ   ‚îî‚îÄ‚îÄ dbt_project/
    ‚îÇ       ‚îú‚îÄ‚îÄ dbt_project.yml
    ‚îÇ       ‚îú‚îÄ‚îÄ profiles.yml
    ‚îÇ       ‚îî‚îÄ‚îÄ models/
    ‚îÇ           ‚îú‚îÄ‚îÄ staging/
    ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ stg_weather.sql
    ‚îÇ           ‚îî‚îÄ‚îÄ marts/
    ‚îÇ               ‚îî‚îÄ‚îÄ daily_weather_stats.sql
    ‚îú‚îÄ‚îÄ api/
    ‚îÇ   ‚îú‚îÄ‚îÄ main.py
    ‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
    ‚îî‚îÄ‚îÄ k8s/
        ‚îú‚îÄ‚îÄ minio.yaml
        ‚îú‚îÄ‚îÄ ingester-cronjob.yaml
        ‚îú‚îÄ‚îÄ transformer-job.yaml
        ‚îî‚îÄ‚îÄ api.yaml
    ```

---

## 1. Stockage objet avec MinIO

Avant que les donn√©es puissent circuler dans notre pipeline, il faut un endroit pour les stocker. En data engineering en production, c'est g√©n√©ralement un objet-store comme **AWS S3** ou **Google Cloud Storage**. Ici, nous allons d√©ployer **MinIO**, un objet-store compatible S3 qui tourne n'importe o√π ‚Äî y compris sur notre cluster Kubernetes local.

MinIO stocke les donn√©es sous forme d'**objets** dans des **buckets**, exactement comme S3. Notre pipeline utilisera un seul bucket `weather` avec deux dossiers :

- `raw/` ‚Äî fichiers JSON bruts tels que re√ßus de l'API
- `transformed/` ‚Äî fichiers Parquet produits par dbt

### a. D√©ployer MinIO

!!! example "Exercice ‚Äî D√©ployer MinIO sur Kubernetes"

    Cr√©ez `k8s/minio.yaml` avec le contenu suivant :

    ```yaml
    # Un PersistentVolumeClaim r√©serve de l'espace disque pour les donn√©es de MinIO.
    # Sans cela, tous les fichiers stock√©s seraient perdus au red√©marrage du pod.
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: minio-pvc
    spec:
      accessModes:
        - ReadWriteOnce          # Un seul pod peut monter ce volume √† la fois
      resources:
        requests:
          storage: 2Gi           # R√©server 2 Go d'espace disque
    ---
    # Un ConfigMap stocke de la configuration non-sensible sous forme de variables d'environnement.
    # Ici, on y d√©finit les identifiants par d√©faut de MinIO.
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: minio-config
    data:
      MINIO_ROOT_USER: "minioadmin"
      MINIO_ROOT_PASSWORD: "minioadmin"
    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: minio
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: minio
      template:
        metadata:
          labels:
            app: minio
        spec:
          containers:
          - name: minio
            image: minio/minio:latest
            imagePullPolicy: IfNotPresent
            args:
              - server                        # Lancer MinIO en mode serveur
              - /data                         # R√©pertoire de donn√©es dans le conteneur
              - --console-address             # Exposer l'interface web sur un port fixe
              - ":9001"
            ports:
            - containerPort: 9000             # Port de l'API S3
            - containerPort: 9001             # Port de l'interface web
            envFrom:
            - configMapRef:
                name: minio-config            # Injecter les identifiants depuis le ConfigMap
            volumeMounts:
            - name: minio-storage
              mountPath: /data                # Monter le PVC sur /data
          volumes:
          - name: minio-storage
            persistentVolumeClaim:
              claimName: minio-pvc
    ---
    # On cr√©e un Service pour MinIO avec deux ports :
    # - Port 9000 : API S3 (utilis√©e en interne par les composants du pipeline)
    # - Port 9001 : Interface web (utilis√©e par nous pour inspecter les buckets)
    apiVersion: v1
    kind: Service
    metadata:
      name: minio-svc
    spec:
      selector:
        app: minio
      ports:
      - name: api
        port: 9000
        targetPort: 9000
      - name: console
        port: 9001
        targetPort: 9001
        nodePort: 30900
      type: NodePort
    ```

    Appliquez le manifeste :

    ```bash
    kubectl apply -f k8s/minio.yaml
    ```

    Attendez que MinIO soit pr√™t :

    ```bash
    kubectl get pods -l app=minio --watch
    # Attendez que le STATUS soit Running, puis Ctrl+C
    ```

    Ouvrez la console web MinIO sur **http://localhost:30900** (ou `minikube service minio-svc` sur Minikube). Connectez-vous avec `minioadmin` / `minioadmin`.

    Cr√©ez un bucket nomm√© **`weather`** en cliquant sur **"Create Bucket"** dans l'interface.

    !!! tip "Ce qu'on vient de d√©ployer"
        - Le **PVC** donne √† MinIO un disque persistant. Si le pod plante et red√©marre, tous les fichiers stock√©s survivent.
        - Le **ConfigMap** externalise les identifiants hors de l'image Docker ‚Äî une bonne pratique Kubernetes.
        - Le **Service NodePort** rend MinIO accessible √† la fois depuis l'int√©rieur du cluster (via `minio-svc:9000`) et depuis notre machine (via `localhost:30900`).

---

## 2. Ingestion des donn√©es avec un CronJob

Maintenant qu'on a un endroit o√π stocker les donn√©es, allons en chercher. Nous allons √©crire un script Python qui appelle l'[API Open-Meteo](https://open-meteo.com/) pour r√©cup√©rer les 7 derniers jours de donn√©es m√©t√©o horaires pour Lyon, puis sauvegarde le r√©sultat en JSON dans notre bucket MinIO.

Nous allons packager ce script dans une image Docker et le faire tourner en tant que **CronJob Kubernetes** ‚Äî une ressource qui ex√©cute un Job selon un planning d√©fini, exactement comme un `cron` Unix.

### a. Le script d'ingestion

!!! example "Exercice ‚Äî Cr√©er le script d'ingestion"

    Cr√©ez `ingester/ingest.py` :

    ```python
    import json
    import os
    from datetime import datetime, timedelta

    import boto3          # SDK AWS ‚Äî fonctionne avec toute API compatible S3, y compris MinIO
    import requests

    # --- Configuration via variables d'environnement ---
    MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "http://minio-svc:9000")
    MINIO_ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY", "minioadmin")
    MINIO_SECRET_KEY = os.getenv("MINIO_SECRET_KEY", "minioadmin")
    BUCKET_NAME = "weather"

    # --- Plage de dates : les 7 derniers jours ---
    end_date = datetime.utcnow().strftime("%Y-%m-%d")
    start_date = (datetime.utcnow() - timedelta(days=7)).strftime("%Y-%m-%d")


    def fetch_weather():
        """R√©cup√®re les donn√©es m√©t√©o horaires pour Lyon depuis Open-Meteo."""
        url = "https://api.open-meteo.com/v1/forecast"
        params = {
            "latitude": 45.75,
            "longitude": 4.85,
            "hourly": "temperature_2m,precipitation,windspeed_10m",
            "start_date": start_date,
            "end_date": end_date,
            "timezone": "Europe/Paris",
        }
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        return response.json()


    def upload_to_minio(data: dict):
        """Upload le payload JSON dans MinIO sous raw/."""
        # boto3 est le client Python standard pour les APIs compatibles S3
        s3 = boto3.client(
            "s3",
            endpoint_url=MINIO_ENDPOINT,
            aws_access_key_id=MINIO_ACCESS_KEY,
            aws_secret_access_key=MINIO_SECRET_KEY,
        )

        filename = f"raw/weather_{datetime.utcnow().strftime('%Y-%m-%dT%H-%M-%S')}.json"
        s3.put_object(
            Bucket=BUCKET_NAME,
            Key=filename,
            Body=json.dumps(data),
            ContentType="application/json",
        )
        print(f"Fichier {filename} upload√© dans le bucket MinIO '{BUCKET_NAME}'")


    if __name__ == "__main__":
        print(f"R√©cup√©ration des donn√©es m√©t√©o du {start_date} au {end_date}...")
        data = fetch_weather()
        print(f"{len(data['hourly']['time'])} enregistrements horaires re√ßus")
        upload_to_minio(data)
        print("Termin√©.")
    ```

    Cr√©ez `ingester/Dockerfile` :

    ```dockerfile
    FROM python:3.11-slim

    WORKDIR /app

    RUN pip install requests boto3

    COPY ingest.py .

    CMD ["python", "ingest.py"]
    ```

    Buildez l'image :

    ```bash
    # Si vous utilisez Minikube, ex√©cutez d'abord : eval $(minikube docker-env)
    docker build -t weather-ingester:1.0.0 ./ingester
    ```

### b. D√©ployer en CronJob

!!! example "Exercice ‚Äî D√©ployer l'ingester en CronJob"

    Cr√©ez `k8s/ingester-cronjob.yaml` :

    ```yaml
    apiVersion: batch/v1
    kind: CronJob
    metadata:
      name: weather-ingester
    spec:
      schedule: "0 * * * *"          # Ex√©cuter au d√©but de chaque heure
      concurrencyPolicy: Forbid       # Ne pas d√©marrer un nouveau Job si le pr√©c√©dent tourne encore
      successfulJobsHistoryLimit: 3   # Conserver les logs des 3 derniers Jobs r√©ussis
      failedJobsHistoryLimit: 1       # Conserver les logs du dernier Job √©chou√©
      jobTemplate:
        spec:
          template:
            spec:
              restartPolicy: OnFailure
              containers:
              - name: ingester
                image: weather-ingester:1.0.0
                imagePullPolicy: IfNotPresent
                env:
                  # On indique √† l'ingester o√π trouver MinIO √† l'int√©rieur du cluster.
                  # Le DNS Kubernetes r√©sout automatiquement "minio-svc" en adresse IP du Service.
                - name: MINIO_ENDPOINT
                  value: "http://minio-svc:9000"
                - name: MINIO_ACCESS_KEY
                  valueFrom:
                    configMapKeyRef:
                      name: minio-config
                      key: MINIO_ROOT_USER
                - name: MINIO_SECRET_KEY
                  valueFrom:
                    configMapKeyRef:
                      name: minio-config
                      key: MINIO_ROOT_PASSWORD
    ```

    Appliquez-le :

    ```bash
    kubectl apply -f k8s/ingester-cronjob.yaml
    ```

    Plut√¥t que d'attendre une heure pour la premi√®re ex√©cution planifi√©e, d√©clenchez manuellement un Job √† la demande :

    ```bash
    kubectl create job weather-ingest-now --from=cronjob/weather-ingester
    ```

    Surveillez l'ex√©cution du Job :

    ```bash
    kubectl get jobs --watch
    # Attendez que COMPLETIONS affiche 1/1, puis Ctrl+C
    ```

    Consultez les logs du Job :

    ```bash
    kubectl logs job/weather-ingest-now
    ```

    Vous devriez voir quelque chose comme :
    ```
    R√©cup√©ration des donn√©es m√©t√©o du 2025-01-15 au 2025-01-22...
    168 enregistrements horaires re√ßus
    Fichier raw/weather_2025-01-22T10-30-00.json upload√© dans le bucket MinIO 'weather'
    Termin√©.
    ```

    Ouvrez l'interface MinIO sur **http://localhost:30900** et naviguez dans le bucket `weather`. Vous devriez voir un fichier sous `raw/`. üéâ

    !!! tip "Concepts cl√©s en action"
        - Le **CronJob** est une ressource Kubernetes qui cr√©e des objets `Job` selon un planning. Un `Job` ex√©cute un pod jusqu'√† sa completion (contrairement √† un Deployment qui tourne ind√©finiment).
        - Le **DNS Kubernetes** signifie que tout pod dans le cluster peut joindre `minio-svc` par son nom ‚Äî pas besoin d'adresses IP. C'est ainsi que les microservices se d√©couvrent mutuellement dans K8s.
        - Le **`configMapKeyRef`** permet d'injecter des valeurs de configuration depuis un ConfigMap comme variables d'environnement, en centralisant les identifiants √† un seul endroit.

---

## 3. Transformation des donn√©es avec dbt

Nous avons maintenant des fichiers JSON bruts qui arrivent dans MinIO toutes les heures. L'√©tape suivante consiste √† les **transformer** en tables propres et agr√©g√©es, pr√™tes pour l'analyse.

[dbt (data build tool)](https://www.getdbt.com/) est l'outil de r√©f√©rence pour les transformations SQL en data engineering. On √©crit des requ√™tes SQL `SELECT` appel√©es **mod√®les**, et dbt se charge de les mat√©rialiser en tables ou vues. Il est con√ßu pour s'ex√©cuter comme un Job batch ‚Äî ce qui correspond parfaitement √† un `Job` Kubernetes.

Notre projet dbt va :
1. Lire les fichiers JSON bruts depuis MinIO gr√¢ce √† **DuckDB** (un moteur SQL embarqu√©) et son extension `httpfs`
2. Parser et nettoyer les donn√©es dans un **mod√®le staging**
3. Agr√©ger les donn√©es en statistiques journali√®res (temp√©rature min/max/moyenne, pr√©cipitations totales) dans un **mod√®le mart**
4. √âcrire les r√©sultats dans MinIO sous forme de fichier Parquet

### a. Le projet dbt

!!! example "Exercice ‚Äî Cr√©er le projet dbt"

    Cr√©ez `transformer/dbt_project/dbt_project.yml` :

    ```yaml
    name: weather_pipeline
    version: "1.0.0"
    profile: weather             # R√©f√©rence le profil dans profiles.yml

    model-paths: ["models"]

    models:
      weather_pipeline:
        staging:
          materialized: view     # Les mod√®les staging sont des vues l√©g√®res
        marts:
          materialized: table    # Les mod√®les marts sont persist√©s en tables
    ```

    Cr√©ez `transformer/dbt_project/profiles.yml` :

    ```yaml
    weather:
      target: dev
      outputs:
        dev:
          type: duckdb
          path: /tmp/weather.duckdb   # Fichier de base DuckDB (temporaire, dans le pod Job)
          extensions:
            - httpfs                  # Permet √† DuckDB de lire/√©crire des fichiers via HTTP/S3
          settings:
            s3_endpoint: "{{ env_var('MINIO_ENDPOINT_HOST') }}"   # ex : minio-svc:9000
            s3_access_key_id: "{{ env_var('MINIO_ACCESS_KEY') }}"
            s3_secret_access_key: "{{ env_var('MINIO_SECRET_KEY') }}"
            s3_use_ssl: "false"
            s3_url_style: "path"      # MinIO utilise les URLs en path-style, pas virtual-hosted
    ```

    Cr√©ez `transformer/dbt_project/models/staging/stg_weather.sql` :

    ```sql
    -- Ce mod√®le staging lit tous les fichiers JSON bruts depuis MinIO
    -- et les aplatit en lignes de (timestamp, temp√©rature, pr√©cipitations, vent).
    --
    -- La fonction read_json_auto() de DuckDB peut lire directement depuis un stockage
    -- compatible S3 gr√¢ce √† l'extension httpfs configur√©e dans profiles.yml.

    WITH raw AS (
        SELECT *
        FROM read_json_auto('s3://weather/raw/*.json')
    ),

    -- L'API Open-Meteo retourne des tableaux de valeurs, une par heure.
    -- On utilise UNNEST pour √©clater ces tableaux en lignes individuelles.
    unnested AS (
        SELECT
            UNNEST(hourly.time)              AS hour_timestamp,
            UNNEST(hourly.temperature_2m)    AS temperature_c,
            UNNEST(hourly.precipitation)     AS precipitation_mm,
            UNNEST(hourly.windspeed_10m)     AS windspeed_kmh
        FROM raw
    )

    SELECT
        hour_timestamp::TIMESTAMP AS hour_timestamp,
        temperature_c,
        precipitation_mm,
        windspeed_kmh
    FROM unnested
    WHERE temperature_c IS NOT NULL
    ```

    Cr√©ez `transformer/dbt_project/models/marts/daily_weather_stats.sql` :

    ```sql
    -- Ce mod√®le mart agr√®ge les donn√©es horaires en statistiques journali√®res.
    -- Il lit depuis le mod√®le staging ci-dessus (ref() est la fa√ßon dont dbt d√©clare les d√©pendances).

    SELECT
        hour_timestamp::DATE            AS date,
        ROUND(MIN(temperature_c), 1)    AS temp_min_c,
        ROUND(MAX(temperature_c), 1)    AS temp_max_c,
        ROUND(AVG(temperature_c), 1)    AS temp_avg_c,
        ROUND(SUM(precipitation_mm), 1) AS total_precipitation_mm,
        ROUND(AVG(windspeed_kmh), 1)    AS avg_windspeed_kmh
    FROM {{ ref('stg_weather') }}
    GROUP BY date
    ORDER BY date DESC
    ```

    Cr√©ez `transformer/Dockerfile` :

    ```dockerfile
    FROM python:3.11-slim

    WORKDIR /app

    # Installer dbt avec l'adaptateur DuckDB
    RUN pip install dbt-duckdb

    COPY dbt_project/ ./dbt_project/

    # Lancer dbt depuis le r√©pertoire du projet
    WORKDIR /app/dbt_project

    # dbt run ex√©cute tous les mod√®les dans l'ordre des d√©pendances
    CMD ["dbt", "run", "--profiles-dir", "."]
    ```

    Buildez l'image :

    ```bash
    docker build -t weather-transformer:1.0.0 ./transformer
    ```

### b. D√©ployer en Job Kubernetes

!!! example "Exercice ‚Äî Ex√©cuter dbt en Job Kubernetes"

    Cr√©ez `k8s/transformer-job.yaml` :

    ```yaml
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: weather-transformer
    spec:
      # backoffLimit d√©finit combien de fois Kubernetes retente le Job en cas d'√©chec
      backoffLimit: 2
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: transformer
            image: weather-transformer:1.0.0
            imagePullPolicy: IfNotPresent
            env:
            - name: MINIO_ENDPOINT_HOST    # Uniquement host:port, sans http:// ‚Äî le format attendu par DuckDB httpfs
              value: "minio-svc:9000"
            - name: MINIO_ACCESS_KEY
              valueFrom:
                configMapKeyRef:
                  name: minio-config
                  key: MINIO_ROOT_USER
            - name: MINIO_SECRET_KEY
              valueFrom:
                configMapKeyRef:
                  name: minio-config
                  key: MINIO_ROOT_PASSWORD
    ```

    Appliquez et ex√©cutez-le :

    ```bash
    kubectl apply -f k8s/transformer-job.yaml
    ```

    Surveillez sa completion :

    ```bash
    kubectl get jobs --watch
    ```

    Consultez la sortie dbt dans les logs :

    ```bash
    kubectl logs job/weather-transformer
    ```

    Vous devriez voir la sortie caract√©ristique de dbt :
    ```
    Running with dbt=1.8.x
    Found 2 models, 0 tests, 0 snapshots

    Concurrency: 1 threads

    1 of 2 START sql view model dev.stg_weather .......................... [RUN]
    1 of 2 OK created sql view model dev.stg_weather ..................... [OK in 1.24s]
    2 of 2 START sql table model dev.marts.daily_weather_stats ........... [RUN]
    2 of 2 OK created sql table model dev.marts.daily_weather_stats ...... [OK in 2.87s]

    Finished running 2 models in 0 hours 0 minutes and 4.11 seconds.
    Completed successfully
    ```

    Ouvrez l'interface MinIO et v√©rifiez qu'un fichier est apparu sous `weather/transformed/`. üéâ

    !!! tip "Job vs CronJob vs Deployment"
        | Ressource | Cas d'usage |
        |---|---|
        | **Pod** | Conteneur nu, sans gestion |
        | **Deployment** | Services long-running (API, serveur web) |
        | **Job** | T√¢ches √† ex√©cution unique (transformation, migration) |
        | **CronJob** | T√¢ches planifi√©es √† ex√©cution unique (ingestion, rapports) |

        dbt est un cas d'usage parfait pour un `Job` : il d√©marre, ex√©cute tous les mod√®les, et se termine. Pas besoin de le maintenir actif entre deux ex√©cutions.

    !!! note "Automatiser le pipeline"
        En production, on cha√Ænerait les √©tapes d'ingestion et de transformation avec un orchestrateur comme **Argo Workflows** ou **Apache Airflow sur Kubernetes**. Pour l'instant, nous les d√©clenchons manuellement en s√©quence. Voir la section "Pour aller plus loin" en fin de TD.

        Vous pouvez d√©j√† simuler une ex√©cution compl√®te du pipeline avec ces deux commandes :

        ```bash
        kubectl create job ingest-$(date +%s) --from=cronjob/weather-ingester
        # Attendez la completion, puis :
        kubectl delete job weather-transformer && kubectl apply -f k8s/transformer-job.yaml
        ```

---

## 4. Exposition des donn√©es avec FastAPI

La transformation est termin√©e et les donn√©es propres sont dans MinIO. La derni√®re pi√®ce du puzzle est de les **servir**. Nous allons d√©ployer une petite application FastAPI qui lit le fichier Parquet transform√© et expose un endpoint `/weather/summary`.

Contrairement √† l'ingester et au transformer qui sont des Jobs √©ph√©m√®res, l'API est un **service long-running** ‚Äî la bonne ressource ici est un `Deployment` avec un `Service` devant lui.

### a. L'API

!!! example "Exercice ‚Äî Cr√©er l'application FastAPI"

    Cr√©ez `api/main.py` :

    ```python
    import os
    import io
    import boto3
    import pandas as pd
    from fastapi import FastAPI, HTTPException

    app = FastAPI(title="Weather Pipeline API", version="1.0.0")

    MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "http://minio-svc:9000")
    MINIO_ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY", "minioadmin")
    MINIO_SECRET_KEY = os.getenv("MINIO_SECRET_KEY", "minioadmin")
    BUCKET_NAME = "weather"
    TRANSFORMED_KEY = "transformed/daily_weather_stats.parquet"


    def get_s3_client():
        return boto3.client(
            "s3",
            endpoint_url=MINIO_ENDPOINT,
            aws_access_key_id=MINIO_ACCESS_KEY,
            aws_secret_access_key=MINIO_SECRET_KEY,
        )


    def load_stats() -> pd.DataFrame:
        """Charge le dernier fichier Parquet transform√© depuis MinIO."""
        s3 = get_s3_client()
        try:
            obj = s3.get_object(Bucket=BUCKET_NAME, Key=TRANSFORMED_KEY)
            return pd.read_parquet(io.BytesIO(obj["Body"].read()))
        except s3.exceptions.NoSuchKey:
            raise HTTPException(
                status_code=404,
                detail="Donn√©es transform√©es introuvables. Ex√©cutez d'abord le Job transformer."
            )


    @app.get("/")
    def root():
        return {"message": "Weather Pipeline API", "docs": "/docs"}


    @app.get("/weather/summary")
    def get_weather_summary():
        """Retourne les statistiques m√©t√©o journali√®res pour Lyon."""
        df = load_stats()
        return df.to_dict(orient="records")


    @app.get("/weather/latest")
    def get_latest_day():
        """Retourne les statistiques du jour le plus r√©cent."""
        df = load_stats()
        return df.iloc[0].to_dict()
    ```

    Cr√©ez `api/Dockerfile` :

    ```dockerfile
    FROM python:3.11-slim

    WORKDIR /app

    RUN pip install fastapi uvicorn boto3 pandas pyarrow

    COPY main.py .

    CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
    ```

    Buildez l'image :

    ```bash
    docker build -t weather-api:1.0.0 ./api
    ```

### b. D√©ployer sur Kubernetes

!!! example "Exercice ‚Äî D√©ployer l'API en Deployment + Service"

    Cr√©ez `k8s/api.yaml` :

    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: weather-api
    spec:
      replicas: 2             # 2 r√©plicas pour la disponibilit√©
      selector:
        matchLabels:
          app: weather-api
      template:
        metadata:
          labels:
            app: weather-api
        spec:
          containers:
          - name: api
            image: weather-api:1.0.0
            imagePullPolicy: IfNotPresent
            ports:
            - containerPort: 8000
            env:
            - name: MINIO_ENDPOINT
              value: "http://minio-svc:9000"
            - name: MINIO_ACCESS_KEY
              valueFrom:
                configMapKeyRef:
                  name: minio-config
                  key: MINIO_ROOT_USER
            - name: MINIO_SECRET_KEY
              valueFrom:
                configMapKeyRef:
                  name: minio-config
                  key: MINIO_ROOT_PASSWORD
            # Les readiness probes indiquent √† Kubernetes quand le conteneur est pr√™t √† recevoir du trafic
            readinessProbe:
              httpGet:
                path: /
                port: 8000
              initialDelaySeconds: 5
              periodSeconds: 5
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: weather-api-svc
    spec:
      selector:
        app: weather-api
      ports:
      - port: 8000
        targetPort: 8000
        nodePort: 30800
      type: NodePort
    ```

    Appliquez-le :

    ```bash
    kubectl apply -f k8s/api.yaml
    ```

    V√©rifiez que les pods sont pr√™ts :

    ```bash
    kubectl get pods -l app=weather-api
    ```

    Ouvrez votre navigateur sur **http://localhost:30800/docs** (ou `minikube service weather-api-svc`).

    Vous devriez voir l'interface Swagger de FastAPI. Testez l'endpoint `/weather/summary` ‚Äî il retourne vos statistiques journali√®res transform√©es par dbt. üéâ

    !!! tip "Le pipeline complet, de bout en bout"
        Prenons du recul et regardons ce qu'on a construit :

        ```
        CronJob (toutes les heures)  ‚Üí  MinIO raw/  ‚Üí  Job (dbt)  ‚Üí  MinIO transformed/  ‚Üí  Deployment (FastAPI)
        ```

        Chaque composant est une **ressource Kubernetes distincte** :
        - Chacun a son propre cycle de vie g√©r√© par Kubernetes
        - Ils communiquent via MinIO (le store de donn√©es partag√©) ‚Äî totalement d√©coupl√©s
        - Si l'API plante, Kubernetes la red√©marre automatiquement (self-healing)
        - On peut scaler l'API √† 10 r√©plicas sans toucher √† l'ingester ni au transformer

        C'est le pattern d'architecture d'un **pipeline ELT batch moderne** sur Kubernetes.

---

## 5. Challenge ‚Äî Supervision avec Prometheus & Grafana

Le pipeline tourne. Comment savoir s'il est en bonne sant√© ? Comment d√©tecter qu'un CronJob √©choue silencieusement depuis 3 heures ?

Dans ce challenge, nous allons ajouter **Prometheus** (collecte de m√©triques) et **Grafana** (visualisation) √† notre cluster gr√¢ce √† **Helm**, le gestionnaire de packages Kubernetes.

!!! note "Installer Helm d'abord"
    Si Helm n'est pas encore install√© :

    ```bash
    # macOS
    brew install helm

    # Windows (avec Chocolatey)
    choco install kubernetes-helm

    # Via script (Linux/macOS)
    curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
    ```

!!! example "Challenge ‚Äî D√©ployer la stack Prometheus + Grafana"

    Ajoutez le d√©p√¥t Helm de la communaut√© :

    ```bash
    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm repo update
    ```

    Installez la stack de supervision compl√®te (Prometheus + Grafana + r√®gles d'alertes) dans un namespace d√©di√© :

    ```bash
    kubectl create namespace monitoring

    helm install monitoring prometheus-community/kube-prometheus-stack \
      --namespace monitoring \
      --set grafana.service.type=NodePort \
      --set grafana.service.nodePort=30300
    ```

    Attendez que tous les pods soient pr√™ts (cela prend une √† deux minutes) :

    ```bash
    kubectl get pods -n monitoring --watch
    ```

    Ouvrez Grafana sur **http://localhost:30300** (identifiants par d√©faut : `admin` / `prom-operator`).

    Dans Grafana, naviguez vers **Dashboards ‚Üí Browse**. Vous trouverez des dashboards pr√©-construits pour :
    - L'utilisation des ressources du cluster Kubernetes (CPU, m√©moire, pods)
    - L'historique de succ√®s/√©chec des CronJobs
    - Les red√©marrages de pods et √©v√©nements de self-healing

    Trouvez le dashboard **"Kubernetes / Jobs"** et cherchez votre CronJob `weather-ingester`. Voyez-vous les derni√®res ex√©cutions r√©ussies ?

    !!! tip "Pourquoi c'est important"
        En production, la supervision n'est pas optionnelle. Un pipeline de donn√©es qui √©choue silencieusement produit des rapports incorrects en aval. Prometheus + Grafana vous donnent une visibilit√© sur :

        - **Taux de succ√®s des Jobs** : mes CronJobs se terminent-ils correctement ?
        - **Dur√©e des Jobs** : l'ingestion prend-elle plus de temps que d'habitude ?
        - **Red√©marrages de pods** : mon API plante-t-elle √† r√©p√©tition ?
        - **Utilisation des ressources** : mon transformer consomme-t-il trop de m√©moire ?

---

## Nettoyage

Quand vous avez termin√©, supprimez toutes les ressources du cluster :

```bash
kubectl delete -f k8s/
kubectl delete namespace monitoring   # Si vous avez fait le challenge supervision
```

---

## Pour aller plus loin

Voici les prochaines √©tapes naturelles pour √©tendre ce pipeline :

**Orchestration de workflows avec Argo Workflows**

Plut√¥t que de d√©clencher les Jobs manuellement, [Argo Workflows](https://argoproj.github.io/workflows/) permet de d√©clarer un DAG (graphe acyclique dirig√©) d'√©tapes directement en YAML Kubernetes. Le Job transformer d√©marrerait automatiquement apr√®s la r√©ussite du Job ingester, avec des retries, des notifications et une interface visuelle.

```yaml
# Un aper√ßu de ce √† quoi ressemble un Argo Workflow :
apiVersion: argoproj.io/v1alpha1
kind: Workflow
spec:
  templates:
  - name: pipeline
    dag:
      tasks:
      - name: ingest
        template: run-ingester
      - name: transform
        template: run-transformer
        dependencies: [ingest]        # Ne d√©marre que si ingest a r√©ussi
```

Puisque vous avez d√©j√† vu **Apache Airflow** dans un module pr√©c√©dent, notez qu'Airflow tourne aussi nativement sur Kubernetes via le `KubernetesExecutor`, qui cr√©e un nouveau pod K8s pour chaque t√¢che. Les deux outils atteignent le m√™me objectif avec des compromis diff√©rents : Argo est plus natif K8s, Airflow est plus familier pour les √©quipes data.

**Qualit√© des donn√©es avec les tests dbt**

dbt dispose d'un [framework de tests int√©gr√©](https://docs.getdbt.com/docs/build/data-tests). Vous pouvez ajouter des tests √† vos mod√®les pour v√©rifier des contraintes de qualit√© des donn√©es :

```yaml
# Dans un fichier schema.yml :
models:
  - name: daily_weather_stats
    columns:
      - name: temp_avg_c
        tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: -30
              max_value: 60
```

**Ingestion en streaming avec Kafka**

L'approche CronJob est de l'ingestion batch ‚Äî on collecte les donn√©es toutes les heures. Pour les pipelines temps r√©el o√π les donn√©es doivent √™tre trait√©es en quelques secondes, [Apache Kafka](https://kafka.apache.org/) est la r√©f√©rence. Kafka tourne aussi sur Kubernetes (via l'op√©rateur Strimzi), et la m√™me architecture MinIO + dbt en aval s'applique.
